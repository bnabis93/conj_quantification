{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import configparser\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model as plotn\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras import models\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './lib/')\n",
    "from help_functions import *\n",
    "\n",
    "from lib.model_lib import *\n",
    "#function to obtain data for training/testing (validation)\n",
    "from extract_patches import get_data_training\n",
    "from extract_patches import get_combine_data_training\n",
    "\n",
    "print(K.tensorflow_backend._get_available_gpus())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom U-net Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.RawConfigParser()\n",
    "config.read('configuration.txt')\n",
    "#patch to the datasets\n",
    "path_data = config.get('data paths', 'path_local')\n",
    "#Experiment name\n",
    "save_folder = config.get('experiment name','result_save_path')\n",
    "save_folder = 'result/'+save_folder\n",
    "name_experiment = config.get('experiment name', 'name')\n",
    "\n",
    "#training settings\n",
    "num_epochs = int(config.get('training settings', 'num_epochs'))\n",
    "batch_size = int(config.get('training settings', 'batch_size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lists = []\n",
    "label_lists = []\n",
    "ratio_lists = [0.4,0.3,0.3]\n",
    "\n",
    "DRIVE_train_imgs_original = config.get('data paths','DRIVE_train_imgs_original')\n",
    "DRIVE_train_groundTruth = config.get('data paths','DRIVE_train_groundTruth')\n",
    "train_lists.append(path_data+DRIVE_train_imgs_original)\n",
    "label_lists.append(path_data+DRIVE_train_groundTruth)\n",
    "\n",
    "\n",
    "#STARE\n",
    "STARE_train_imgs_original = config.get('data paths','STARE_train_imgs_original')\n",
    "STARE_train_groundTruth = config.get('data paths','STARE_train_groundTruth')\n",
    "train_lists.append(path_data+STARE_train_imgs_original)\n",
    "label_lists.append(path_data+STARE_train_groundTruth)\n",
    "\n",
    "\n",
    "#CHASE\n",
    "CHASE_train_imgs_original = config.get('data paths','CHASE_train_imgs_original')\n",
    "CHASE_train_groundTruth = config.get('data paths','CHASE_train_groundTruth')\n",
    "train_lists.append(path_data+CHASE_train_imgs_original)\n",
    "label_lists.append(path_data+CHASE_train_groundTruth)\n",
    "\n",
    "#HRF\n",
    "HRF_train_imgs_original = config.get('data paths','HRF_train_imgs_original')\n",
    "HRF_train_groundTruth = config.get('data paths','HRF_train_groundTruth')\n",
    "\n",
    "#train_lists.append(path_data+HRF_train_imgs_original)\n",
    "#label_lists.append(path_data+HRF_train_groundTruth)\n",
    "\n",
    "#Fixed CONJ\n",
    "CONJ_train_imgs_original = config.get('data paths','CONJ_train_imgs_original')\n",
    "CONJ_train_groundTruth = config.get('data paths','CONJ_train_groundTruth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figure5_augReduceLr_conjHrf\n",
      "['./hdf5_datasets_training_testing/DRIVE/DRIVE_dataset_imgs_train.hdf5', './hdf5_datasets_training_testing/STARE/STARE_dataset_imgs_train.hdf5', './hdf5_datasets_training_testing/CHASE_DB/CHASE_dataset_imgs_train.hdf5']\n",
      "3\n",
      "DRIVE\n",
      "['.', 'hdf5_datasets_training_testing', 'DRIVE', 'DRIVE_dataset_groundTruth_train.hdf5']\n"
     ]
    }
   ],
   "source": [
    "print(name_experiment)\n",
    "print(train_lists)\n",
    "print(len(train_lists))\n",
    "\n",
    "temp = train_lists[0].split('/')\n",
    "print(temp[2])\n",
    "\n",
    "print(label_lists[0].split('/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist the folder in this path : ./result/figure5/figure5_aug400000_batch16_lr_conjHrf\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir('./'+save_folder+'/'+name_experiment) == False:\n",
    "    os.mkdir('./'+save_folder+'/'+name_experiment)\n",
    "else:\n",
    "    print('already exist the folder in this path : {}'.format('./'+save_folder+'/'+name_experiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_lists' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a9baa7dbe498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m DB_patches, key_train, key_label = get_combine_data_training(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlist_train_imgs_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lists\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlist_train_groudTruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_lists\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m#masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpatch_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data attributes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'patch_height'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpatch_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data attributes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'patch_width'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_lists' is not defined"
     ]
    }
   ],
   "source": [
    "DB_patches, key_train, key_label = get_combine_data_training(\n",
    "    list_train_imgs_original = train_lists, \n",
    "    list_train_groudTruth = label_lists,  #masks\n",
    "    patch_height = int(config.get('data attributes', 'patch_height')),\n",
    "    patch_width = int(config.get('data attributes', 'patch_width')),\n",
    "    patch_ratio_list = ratio_lists,\n",
    "    num_subimgs = int(config.get('training settings', 'num_subimgs')),\n",
    "    inside_FOV = config.getboolean('training settings', 'inside_FOV'), #select the patches only inside the FOV  (default == True)\n",
    "    save_path = save_folder+'/'+name_experiment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DB_patches.keys())\n",
    "print(key_train)\n",
    "print(key_label)\n",
    "print(np.shape(DB_patches['DRIVE_train']))\n",
    "print(np.shape(DB_patches['DRIVE_label']))\n",
    "print(np.shape(DB_patches['STARE_train']))\n",
    "print(np.shape(DB_patches['STARE_label']))\n",
    "print(np.shape(DB_patches['CHASE_DB_train']))\n",
    "print(np.shape(DB_patches['CHASE_DB_label']))\n",
    "\n",
    "patches_imgs_train = np.concatenate((DB_patches['DRIVE_train'], DB_patches['STARE_train']),axis = 0)\n",
    "patches_imgs_train = np.concatenate((patches_imgs_train, DB_patches['CHASE_DB_train']),axis = 0)\n",
    "\n",
    "patches_masks_train = np.concatenate((DB_patches['DRIVE_label'], DB_patches['STARE_label']),axis = 0)\n",
    "patches_masks_train = np.concatenate((patches_masks_train, DB_patches['CHASE_DB_label']),axis = 0)\n",
    "\n",
    "print(np.shape(patches_imgs_train))\n",
    "print(np.shape(patches_masks_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract patch for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist the folder in this path : result/figure5/figure5_aug400000_batch16_lr_conjHrf\n",
      "number of subimages :  400000\n",
      "[DEBUG] shape of train_imgs_original :  (25, 3, 900, 1100)\n",
      "[DEBUG] shape of train_imgs_label :  (25, 1, 900, 1100)\n",
      "[DEBUG] normalize shape :  (25, 1, 900, 1100)\n",
      "[DEBUG] i normalize shape :  (1, 900, 1100)\n",
      "[get_data_training] preprocessed image shape :  (25, 1, 900, 1100)\n",
      "train image max :  1.0 train image min :  0.0\n",
      "255.0\n",
      "[get_data_training] preprocessed2 image shape :  (25, 1, 900, 1100)\n",
      "1.0\n",
      "\n",
      "[get_data_training] train images/masks shape : (25, 1, 900, 1100)\n",
      "[get_data_training] train images range (min-max) [0.0 , 1.0] \n",
      "[get_data_training] train masks are within 0-1\n",
      "\n",
      "Maximum subsample size :  866096\n",
      "[extract random] patches shape : (400000, 1, 64, 64)\n",
      "[extract random] patches per full image : 16000\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 1)\n",
      "\n",
      "[get_data_training] train PATCHES images/masks shape : (400000, 1, 64, 64)\n",
      "[get_data_training] train PATCHES images range (min-max): 0.0 - 1.0\n",
      "[get_data_training] patches_imgs_train : (400000, 1, 64, 64)\n",
      "[group images func] prev data shape  : (50, 1, 64, 64)\n",
      "[group images func] after data shape :  (50, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 320, 1)\n",
      "[group images func] final total image :  (704, 320, 1)\n",
      "data shape :  (704, 320, 1)\n",
      "<PIL.Image.Image image mode=L size=320x704 at 0x7F5DE07E5EF0>\n",
      "file name :  ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/train_patch_img\n",
      "[Augmentation function] patches shape :  (400000, 1, 64, 64)\n",
      "[Augmentation function] augmentation patches shape :  (400000, 64, 64, 1)\n",
      "[augmentation] patches_imgs_train : (400000, 1, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "patches_imgs_train, patches_masks_train = get_data_training(\n",
    "    DRIVE_train_imgs_original = path_data + config.get('data paths', 'CONJ_train_imgs_original'),\n",
    "    DRIVE_train_groudTruth = path_data + config.get('data paths', 'CONJ_train_groundTruth'),  #masks\n",
    "    patch_height = int(config.get('data attributes', 'patch_height')),\n",
    "    patch_width = int(config.get('data attributes', 'patch_width')),\n",
    "    num_subimgs = int(config.get('training settings', 'num_subimgs')),\n",
    "    inside_FOV = config.getboolean('training settings', 'inside_FOV'), #select the patches only inside the FOV  (default == True)\n",
    "    save_path = save_folder+'/'+name_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 1, 64, 64)\n",
      "0.4188419117647059\n",
      "400000\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(patches_imgs_train))\n",
    "print(np.max(patches_imgs_train[1]))\n",
    "print(len(patches_imgs_train))\n",
    "print(np.max(patches_masks_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[group images func] prev data shape  : (40, 1, 64, 64)\n",
      "[group images func] after data shape :  (40, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 320, 1)\n",
      "[group images func] final total image :  (576, 320, 1)\n",
      "data shape :  (576, 320, 1)\n",
      "<PIL.Image.Image image mode=L size=320x576 at 0x7F5D52162160>\n",
      "file name :  ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/sample_input_imgs\n",
      "[group images func] prev data shape  : (40, 1, 64, 64)\n",
      "[group images func] after data shape :  (40, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 320, 1)\n",
      "[group images func] final total image :  (576, 320, 1)\n",
      "data shape :  (576, 320, 1)\n",
      "<PIL.Image.Image image mode=L size=320x576 at 0x7F5D52162208>\n",
      "file name :  ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/sample_input_masks\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=320x576 at 0x7F5D52162208>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_sample = min(patches_imgs_train.shape[0],40)\n",
    "visualize(group_images(patches_imgs_train[0:N_sample,:,:,:]* 255,5),'./'+save_folder+'/'+name_experiment+'/'+\"sample_input_imgs\")#.show()\n",
    "visualize(group_images(patches_masks_train[0:N_sample,:,:,:] * 255,5),'./'+save_folder+'/'+name_experiment+'/'+\"sample_input_masks\")#.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### data generator test\n",
    "\n",
    "\n",
    "'./'+save_folder+'/'+name_experiment +name_experiment+ '_model.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data gen test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape :  (None, 1, 64, 64)\n",
      "\n",
      "gating shape : (None, 128, 4, 4), conv4 shape : (None, 64, 8, 8)\n",
      "shape x,g  (None, 64, 8, 8) (None, 128, 4, 4)\n",
      "inter shape :   128\n",
      "stride x : 1 stride y : 1\n",
      "theta_x shape :  (None, 128, 4, 4)\n",
      "upsample_g shape :  (None, 128, 4, 4)\n",
      "\n",
      "attn1 shape : (None, 64, 8, 8) center shape : (None, 128, 4, 4) \n",
      "\n",
      "attn1 shape : (None, 64, 8, 8) up1 shape : (None, 128, 8, 8)\n",
      "shape x,g  (None, 64, 16, 16) (None, 128, 8, 8)\n",
      "inter shape :   64\n",
      "stride x : 1 stride y : 1\n",
      "theta_x shape :  (None, 64, 8, 8)\n",
      "upsample_g shape :  (None, 64, 8, 8)\n",
      "shape x,g  (None, 32, 32, 32) (None, 128, 16, 16)\n",
      "inter shape :   64\n",
      "stride x : 1 stride y : 1\n",
      "theta_x shape :  (None, 64, 16, 16)\n",
      "upsample_g shape :  (None, 64, 16, 16)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 64, 64)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 64, 64)   320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 64, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 64, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 64, 64)   9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 64, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 32)   9248        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   9248        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 16, 16)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 16, 16)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 16, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 16, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 16, 16)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 16, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 16, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 8, 8)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 8, 8)     36928       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 8, 8)     32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 8, 8)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 8, 8)     36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 8, 8)     32          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 8, 8)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 64, 4, 4)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 128, 4, 4)    73856       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 128, 4, 4)    16          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 128, 4, 4)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 128, 4, 4)    147584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 128, 4, 4)    16          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 128, 4, 4)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "gating01_conv (Conv2D)          (None, 128, 4, 4)    16512       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating01_bn (BatchNormalization (None, 128, 4, 4)    16          gating01_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating01_act (Activation)       (None, 128, 4, 4)    0           gating01_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 128, 4, 4)    16512       gating01_act[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "g_upattn01 (Conv2DTranspose)    (None, 128, 4, 4)    147584      conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "xlattn01 (Conv2D)               (None, 128, 4, 4)    32896       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128, 4, 4)    0           g_upattn01[0][0]                 \n",
      "                                                                 xlattn01[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 128, 4, 4)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "psiattn01 (Conv2D)              (None, 1, 4, 4)      129         activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 1, 4, 4)      0           psiattn01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 1, 8, 8)      0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "psi_upattn01 (Lambda)           (None, 64, 8, 8)     0           up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_attnattn01 (Multiply)         (None, 64, 8, 8)     0           psi_upattn01[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_convattn01 (Conv2D)      (None, 64, 8, 8)     4160        q_attnattn01[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 64, 8, 8)     73792       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_bnattn01 (BatchNormaliza (None, 64, 8, 8)     32          q_attn_convattn01[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 8, 8)    0           conv2d_transpose_1[0][0]         \n",
      "                                                                 q_attn_bnattn01[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gating02_conv (Conv2D)          (None, 128, 8, 8)    16512       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating02_bn (BatchNormalization (None, 128, 8, 8)    32          gating02_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating02_act (Activation)       (None, 128, 8, 8)    0           gating02_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 8, 8)     8256        gating02_act[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "g_upattn02 (Conv2DTranspose)    (None, 64, 8, 8)     36928       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "xlattn02 (Conv2D)               (None, 64, 8, 8)     16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 8, 8)     0           g_upattn02[0][0]                 \n",
      "                                                                 xlattn02[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 8, 8)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "psiattn02 (Conv2D)              (None, 1, 8, 8)      65          activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1, 8, 8)      0           psiattn02[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 1, 16, 16)    0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "psi_upattn02 (Lambda)           (None, 64, 16, 16)   0           up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_attnattn02 (Multiply)         (None, 64, 16, 16)   0           psi_upattn02[0][0]               \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_convattn02 (Conv2D)      (None, 64, 16, 16)   4160        q_attnattn02[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 16, 16)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_bnattn02 (BatchNormaliza (None, 64, 16, 16)   64          q_attn_convattn02[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 16, 16)  0           conv2d_transpose_2[0][0]         \n",
      "                                                                 q_attn_bnattn02[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gating03_conv (Conv2D)          (None, 128, 16, 16)  16512       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating03_bn (BatchNormalization (None, 128, 16, 16)  64          gating03_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating03_act (Activation)       (None, 128, 16, 16)  0           gating03_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 16, 16)   8256        gating03_act[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "g_upattn03 (Conv2DTranspose)    (None, 64, 16, 16)   36928       conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "xlattn03 (Conv2D)               (None, 64, 16, 16)   8256        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 16, 16)   0           g_upattn03[0][0]                 \n",
      "                                                                 xlattn03[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 16, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "psiattn03 (Conv2D)              (None, 1, 16, 16)    65          activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 1, 16, 16)    0           psiattn03[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 1, 32, 32)    0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "psi_upattn03 (Lambda)           (None, 32, 32, 32)   0           up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_attnattn03 (Multiply)         (None, 32, 32, 32)   0           psi_upattn03[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_convattn03 (Conv2D)      (None, 32, 32, 32)   1056        q_attnattn03[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 32, 32, 32)   36896       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_bnattn03 (BatchNormaliza (None, 32, 32, 32)   128         q_attn_convattn03[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 32, 32)   0           conv2d_transpose_3[0][0]         \n",
      "                                                                 q_attn_bnattn03[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 32, 64, 64)   18464       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64, 64, 64)   0           conv2d_transpose_4[0][0]         \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 2, 64, 64)    130         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 2, 4096)      0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 4096, 2)      0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 4096, 2)      0           permute_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 954,421\n",
      "Trainable params: 953,757\n",
      "Non-trainable params: 664\n",
      "__________________________________________________________________________________________________\n",
      "Check: final output of the network:\n",
      "(None, 4096, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41700"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ch = patches_imgs_train.shape[1]\n",
    "patch_height = patches_imgs_train.shape[2]\n",
    "patch_width = patches_imgs_train.shape[3]\n",
    "\n",
    "\n",
    "#model = naive_attn_unet(n_ch, patch_height, patch_width)  #the U-net model\n",
    "#model = unet(n_ch, patch_height, patch_width)\n",
    "model = attn_unet(n_ch, patch_height, patch_width,2)\n",
    "#model = small_attn_unet(n_ch, patch_height, patch_width,2)\n",
    "print (\"Check: final output of the network:\")\n",
    "print (model.output_shape)\n",
    "\n",
    "plot(model, to_file= './'+save_folder+'/'+name_experiment+'/' +name_experiment+ '_model.png')   #check how the model looks like\n",
    "#plot(model, to_file= name_experiment+'/'+name_experiment + '_model.png')   #check how the model looks like\n",
    "\n",
    "json_string = model.to_json()\n",
    "open('./'+save_folder+'/'+name_experiment+'/' +name_experiment+'_architecture.json', 'w').write(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=40, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, verbose=1)\n",
    "\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "    \"\"\"Cosine annealing scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # 1e-9 + (1e-6 ) * (1 + cos ())\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(int(config.get('training settings', 'num_subimgs')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training session] before mask unet func patch mask shape :  (400000, 1, 64, 64)\n",
      "[training session] after mask unet func patch mask shape :  (400000, 4096, 2)\n",
      "Train on 320000 samples, validate on 80000 samples\n",
      "Epoch 1/150\n",
      "320000/320000 [==============================] - 1565s 5ms/step - loss: 0.1153 - dice_coef: 0.8847 - val_loss: 0.0756 - val_dice_coef: 0.9244\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07558, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 2/150\n",
      "320000/320000 [==============================] - 1752s 5ms/step - loss: 0.0674 - dice_coef: 0.9326 - val_loss: 0.0629 - val_dice_coef: 0.9371\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07558 to 0.06287, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 3/150\n",
      "320000/320000 [==============================] - 1703s 5ms/step - loss: 0.0578 - dice_coef: 0.9422 - val_loss: 0.0577 - val_dice_coef: 0.9423\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06287 to 0.05773, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 4/150\n",
      "320000/320000 [==============================] - 1715s 5ms/step - loss: 0.0548 - dice_coef: 0.9452 - val_loss: 0.0556 - val_dice_coef: 0.9444\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05773 to 0.05560, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 5/150\n",
      "320000/320000 [==============================] - 1753s 5ms/step - loss: 0.0518 - dice_coef: 0.9482 - val_loss: 0.0520 - val_dice_coef: 0.9480\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05560 to 0.05203, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 6/150\n",
      "320000/320000 [==============================] - 1864s 6ms/step - loss: 0.0503 - dice_coef: 0.9497 - val_loss: 0.0514 - val_dice_coef: 0.9486\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.05203 to 0.05139, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 7/150\n",
      "320000/320000 [==============================] - 1806s 6ms/step - loss: 0.0496 - dice_coef: 0.9504 - val_loss: 0.0509 - val_dice_coef: 0.9491\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.05139 to 0.05089, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 8/150\n",
      "320000/320000 [==============================] - 1937s 6ms/step - loss: 0.0491 - dice_coef: 0.9509 - val_loss: 0.0504 - val_dice_coef: 0.9496\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.05089 to 0.05041, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 9/150\n",
      "320000/320000 [==============================] - 1816s 6ms/step - loss: 0.0487 - dice_coef: 0.9513 - val_loss: 0.0505 - val_dice_coef: 0.9495\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.05041\n",
      "Epoch 10/150\n",
      "320000/320000 [==============================] - 1923s 6ms/step - loss: 0.0483 - dice_coef: 0.9517 - val_loss: 0.0498 - val_dice_coef: 0.9502\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.05041 to 0.04976, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 11/150\n",
      "320000/320000 [==============================] - 1826s 6ms/step - loss: 0.0479 - dice_coef: 0.9521 - val_loss: 0.0500 - val_dice_coef: 0.9500\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.04976\n",
      "Epoch 12/150\n",
      "320000/320000 [==============================] - 1963s 6ms/step - loss: 0.0476 - dice_coef: 0.9524 - val_loss: 0.0501 - val_dice_coef: 0.9499\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.04976\n",
      "Epoch 13/150\n",
      "320000/320000 [==============================] - 1827s 6ms/step - loss: 0.0473 - dice_coef: 0.9527 - val_loss: 0.0496 - val_dice_coef: 0.9504\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.04976 to 0.04962, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 14/150\n",
      "320000/320000 [==============================] - 1946s 6ms/step - loss: 0.0470 - dice_coef: 0.9530 - val_loss: 0.0496 - val_dice_coef: 0.9504\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.04962 to 0.04960, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 15/150\n",
      "320000/320000 [==============================] - 1832s 6ms/step - loss: 0.0467 - dice_coef: 0.9533 - val_loss: 0.0486 - val_dice_coef: 0.9514\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.04960 to 0.04863, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 16/150\n",
      "320000/320000 [==============================] - 1958s 6ms/step - loss: 0.0464 - dice_coef: 0.9536 - val_loss: 0.0490 - val_dice_coef: 0.9510\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.04863\n",
      "Epoch 17/150\n",
      "320000/320000 [==============================] - 1831s 6ms/step - loss: 0.0461 - dice_coef: 0.9539 - val_loss: 0.0483 - val_dice_coef: 0.9517\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.04863 to 0.04834, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 18/150\n",
      "320000/320000 [==============================] - 2008s 6ms/step - loss: 0.0459 - dice_coef: 0.9541 - val_loss: 0.0488 - val_dice_coef: 0.9512\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.04834\n",
      "Epoch 19/150\n",
      "320000/320000 [==============================] - 1917s 6ms/step - loss: 0.0456 - dice_coef: 0.9544 - val_loss: 0.0487 - val_dice_coef: 0.9513\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.04834\n",
      "Epoch 20/150\n",
      "320000/320000 [==============================] - 1975s 6ms/step - loss: 0.0453 - dice_coef: 0.9547 - val_loss: 0.0482 - val_dice_coef: 0.9518\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.04834 to 0.04823, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 21/150\n",
      "320000/320000 [==============================] - 1873s 6ms/step - loss: 0.0451 - dice_coef: 0.9549 - val_loss: 0.0480 - val_dice_coef: 0.9520\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.04823 to 0.04799, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 22/150\n",
      "320000/320000 [==============================] - 1982s 6ms/step - loss: 0.0449 - dice_coef: 0.9551 - val_loss: 0.0483 - val_dice_coef: 0.9517\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.04799\n",
      "Epoch 23/150\n",
      "320000/320000 [==============================] - 1876s 6ms/step - loss: 0.0446 - dice_coef: 0.9554 - val_loss: 0.0477 - val_dice_coef: 0.9523\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.04799 to 0.04769, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 24/150\n",
      "320000/320000 [==============================] - 1970s 6ms/step - loss: 0.0444 - dice_coef: 0.9556 - val_loss: 0.0476 - val_dice_coef: 0.9524\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.04769 to 0.04755, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 25/150\n",
      "320000/320000 [==============================] - 1863s 6ms/step - loss: 0.0442 - dice_coef: 0.9558 - val_loss: 0.0475 - val_dice_coef: 0.9525\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.04755 to 0.04751, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 26/150\n",
      "320000/320000 [==============================] - 1941s 6ms/step - loss: 0.0441 - dice_coef: 0.9559 - val_loss: 0.0474 - val_dice_coef: 0.9526\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.04751 to 0.04739, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 27/150\n",
      "320000/320000 [==============================] - 1869s 6ms/step - loss: 0.0439 - dice_coef: 0.9561 - val_loss: 0.0472 - val_dice_coef: 0.9528\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.04739 to 0.04722, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 28/150\n",
      "320000/320000 [==============================] - 1969s 6ms/step - loss: 0.0437 - dice_coef: 0.9563 - val_loss: 0.0470 - val_dice_coef: 0.9530\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.04722 to 0.04701, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 29/150\n",
      "320000/320000 [==============================] - 1863s 6ms/step - loss: 0.0435 - dice_coef: 0.9565 - val_loss: 0.0468 - val_dice_coef: 0.9532\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.04701 to 0.04677, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 30/150\n",
      "320000/320000 [==============================] - 1935s 6ms/step - loss: 0.0434 - dice_coef: 0.9566 - val_loss: 0.0468 - val_dice_coef: 0.9532\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.04677\n",
      "Epoch 31/150\n",
      "320000/320000 [==============================] - 1822s 6ms/step - loss: 0.0433 - dice_coef: 0.9567 - val_loss: 0.0469 - val_dice_coef: 0.9531\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.04677\n",
      "Epoch 32/150\n",
      "320000/320000 [==============================] - 1944s 6ms/step - loss: 0.0431 - dice_coef: 0.9569 - val_loss: 0.0467 - val_dice_coef: 0.9533\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.04677 to 0.04675, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 33/150\n",
      "320000/320000 [==============================] - 1855s 6ms/step - loss: 0.0430 - dice_coef: 0.9570 - val_loss: 0.0463 - val_dice_coef: 0.9537\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.04675 to 0.04628, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 34/150\n",
      "320000/320000 [==============================] - 1959s 6ms/step - loss: 0.0429 - dice_coef: 0.9571 - val_loss: 0.0469 - val_dice_coef: 0.9531\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.04628\n",
      "Epoch 35/150\n",
      "320000/320000 [==============================] - 1831s 6ms/step - loss: 0.0427 - dice_coef: 0.9573 - val_loss: 0.0463 - val_dice_coef: 0.9537\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.04628\n",
      "Epoch 36/150\n",
      "320000/320000 [==============================] - 1934s 6ms/step - loss: 0.0426 - dice_coef: 0.9574 - val_loss: 0.0463 - val_dice_coef: 0.9537\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.04628 to 0.04628, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 37/150\n",
      "320000/320000 [==============================] - 1817s 6ms/step - loss: 0.0425 - dice_coef: 0.9575 - val_loss: 0.0461 - val_dice_coef: 0.9539\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.04628 to 0.04609, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 38/150\n",
      "320000/320000 [==============================] - 1940s 6ms/step - loss: 0.0424 - dice_coef: 0.9576 - val_loss: 0.0462 - val_dice_coef: 0.9538\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.04609\n",
      "Epoch 39/150\n",
      "320000/320000 [==============================] - 1838s 6ms/step - loss: 0.0423 - dice_coef: 0.9577 - val_loss: 0.0460 - val_dice_coef: 0.9540\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.04609 to 0.04597, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 40/150\n",
      "320000/320000 [==============================] - 1949s 6ms/step - loss: 0.0422 - dice_coef: 0.9578 - val_loss: 0.0459 - val_dice_coef: 0.9541\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.04597 to 0.04593, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 41/150\n",
      "320000/320000 [==============================] - 1827s 6ms/step - loss: 0.0421 - dice_coef: 0.9579 - val_loss: 0.0458 - val_dice_coef: 0.9542\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.04593 to 0.04577, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 42/150\n",
      "320000/320000 [==============================] - 1924s 6ms/step - loss: 0.0420 - dice_coef: 0.9580 - val_loss: 0.0457 - val_dice_coef: 0.9543\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.04577 to 0.04569, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 43/150\n",
      "320000/320000 [==============================] - 1826s 6ms/step - loss: 0.0419 - dice_coef: 0.9581 - val_loss: 0.0459 - val_dice_coef: 0.9541\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.04569\n",
      "Epoch 44/150\n",
      "320000/320000 [==============================] - 1951s 6ms/step - loss: 0.0418 - dice_coef: 0.9582 - val_loss: 0.0455 - val_dice_coef: 0.9545\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.04569 to 0.04551, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 45/150\n",
      "320000/320000 [==============================] - 1839s 6ms/step - loss: 0.0417 - dice_coef: 0.9583 - val_loss: 0.0457 - val_dice_coef: 0.9543\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.04551\n",
      "Epoch 46/150\n",
      "320000/320000 [==============================] - 1932s 6ms/step - loss: 0.0416 - dice_coef: 0.9584 - val_loss: 0.0454 - val_dice_coef: 0.9546\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.04551 to 0.04543, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 47/150\n",
      "320000/320000 [==============================] - 1827s 6ms/step - loss: 0.0415 - dice_coef: 0.9585 - val_loss: 0.0455 - val_dice_coef: 0.9545\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.04543\n",
      "Epoch 48/150\n",
      "320000/320000 [==============================] - 1940s 6ms/step - loss: 0.0415 - dice_coef: 0.9585 - val_loss: 0.0457 - val_dice_coef: 0.9543\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.04543\n",
      "Epoch 49/150\n",
      "320000/320000 [==============================] - 1829s 6ms/step - loss: 0.0414 - dice_coef: 0.9586 - val_loss: 0.0453 - val_dice_coef: 0.9547\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.04543 to 0.04533, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 50/150\n",
      "320000/320000 [==============================] - 1913s 6ms/step - loss: 0.0413 - dice_coef: 0.9587 - val_loss: 0.0451 - val_dice_coef: 0.9549\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.04533 to 0.04507, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 51/150\n",
      "320000/320000 [==============================] - 1814s 6ms/step - loss: 0.0413 - dice_coef: 0.9587 - val_loss: 0.0453 - val_dice_coef: 0.9547\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.04507\n",
      "Epoch 52/150\n",
      "320000/320000 [==============================] - 1925s 6ms/step - loss: 0.0412 - dice_coef: 0.9588 - val_loss: 0.0455 - val_dice_coef: 0.9545\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.04507\n",
      "Epoch 53/150\n",
      "320000/320000 [==============================] - 1818s 6ms/step - loss: 0.0411 - dice_coef: 0.9589 - val_loss: 0.0451 - val_dice_coef: 0.9549\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.04507\n",
      "Epoch 54/150\n",
      "320000/320000 [==============================] - 1915s 6ms/step - loss: 0.0411 - dice_coef: 0.9589 - val_loss: 0.0451 - val_dice_coef: 0.9549\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.04507 to 0.04507, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 55/150\n",
      "320000/320000 [==============================] - 1813s 6ms/step - loss: 0.0410 - dice_coef: 0.9590 - val_loss: 0.0450 - val_dice_coef: 0.9550\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.04507 to 0.04504, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 56/150\n",
      "320000/320000 [==============================] - 1921s 6ms/step - loss: 0.0409 - dice_coef: 0.9591 - val_loss: 0.0451 - val_dice_coef: 0.9549\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.04504\n",
      "Epoch 57/150\n",
      "320000/320000 [==============================] - 1819s 6ms/step - loss: 0.0409 - dice_coef: 0.9591 - val_loss: 0.0449 - val_dice_coef: 0.9551\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.04504 to 0.04493, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 58/150\n",
      "320000/320000 [==============================] - 1915s 6ms/step - loss: 0.0408 - dice_coef: 0.9592 - val_loss: 0.0449 - val_dice_coef: 0.9551\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.04493 to 0.04488, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 59/150\n",
      "320000/320000 [==============================] - 1812s 6ms/step - loss: 0.0408 - dice_coef: 0.9592 - val_loss: 0.0452 - val_dice_coef: 0.9548\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.04488\n",
      "Epoch 60/150\n",
      "320000/320000 [==============================] - 1916s 6ms/step - loss: 0.0407 - dice_coef: 0.9593 - val_loss: 0.0451 - val_dice_coef: 0.9549\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.04488\n",
      "Epoch 61/150\n",
      "320000/320000 [==============================] - 1811s 6ms/step - loss: 0.0407 - dice_coef: 0.9593 - val_loss: 0.0448 - val_dice_coef: 0.9552\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.04488 to 0.04478, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 62/150\n",
      "320000/320000 [==============================] - 1925s 6ms/step - loss: 0.0406 - dice_coef: 0.9594 - val_loss: 0.0448 - val_dice_coef: 0.9552\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.04478\n",
      "Epoch 63/150\n",
      "320000/320000 [==============================] - 1816s 6ms/step - loss: 0.0406 - dice_coef: 0.9594 - val_loss: 0.0448 - val_dice_coef: 0.9552\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.04478 to 0.04475, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 64/150\n",
      "320000/320000 [==============================] - 1930s 6ms/step - loss: 0.0405 - dice_coef: 0.9595 - val_loss: 0.0448 - val_dice_coef: 0.9552\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.04475\n",
      "Epoch 65/150\n",
      "320000/320000 [==============================] - 1811s 6ms/step - loss: 0.0405 - dice_coef: 0.9595 - val_loss: 0.0448 - val_dice_coef: 0.9552\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.04475\n",
      "Epoch 66/150\n",
      "320000/320000 [==============================] - 1929s 6ms/step - loss: 0.0404 - dice_coef: 0.9596 - val_loss: 0.0446 - val_dice_coef: 0.9554\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.04475 to 0.04465, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 67/150\n",
      "320000/320000 [==============================] - 1821s 6ms/step - loss: 0.0404 - dice_coef: 0.9596 - val_loss: 0.0448 - val_dice_coef: 0.9552\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.04465\n",
      "Epoch 68/150\n",
      "320000/320000 [==============================] - 1923s 6ms/step - loss: 0.0403 - dice_coef: 0.9597 - val_loss: 0.0447 - val_dice_coef: 0.9553\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.04465\n",
      "Epoch 69/150\n",
      "320000/320000 [==============================] - 1819s 6ms/step - loss: 0.0403 - dice_coef: 0.9597 - val_loss: 0.0447 - val_dice_coef: 0.9553\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.04465\n",
      "Epoch 70/150\n",
      "320000/320000 [==============================] - 1942s 6ms/step - loss: 0.0402 - dice_coef: 0.9598 - val_loss: 0.0446 - val_dice_coef: 0.9554\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.04465 to 0.04457, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 71/150\n",
      "320000/320000 [==============================] - 1839s 6ms/step - loss: 0.0402 - dice_coef: 0.9598 - val_loss: 0.0449 - val_dice_coef: 0.9551\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.04457\n",
      "Epoch 72/150\n",
      "320000/320000 [==============================] - 1904s 6ms/step - loss: 0.0401 - dice_coef: 0.9599 - val_loss: 0.0449 - val_dice_coef: 0.9551\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.04457\n",
      "Epoch 73/150\n",
      "320000/320000 [==============================] - 1813s 6ms/step - loss: 0.0401 - dice_coef: 0.9599 - val_loss: 0.0446 - val_dice_coef: 0.9554\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.04457\n",
      "Epoch 74/150\n",
      "320000/320000 [==============================] - 1938s 6ms/step - loss: 0.0401 - dice_coef: 0.9599 - val_loss: 0.0444 - val_dice_coef: 0.9556\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.04457 to 0.04444, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 75/150\n",
      "320000/320000 [==============================] - 1837s 6ms/step - loss: 0.0400 - dice_coef: 0.9600 - val_loss: 0.0444 - val_dice_coef: 0.9556\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.04444\n",
      "Epoch 76/150\n",
      "320000/320000 [==============================] - 1924s 6ms/step - loss: 0.0400 - dice_coef: 0.9600 - val_loss: 0.0445 - val_dice_coef: 0.9555\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.04444\n",
      "Epoch 77/150\n",
      "320000/320000 [==============================] - 1807s 6ms/step - loss: 0.0399 - dice_coef: 0.9601 - val_loss: 0.0445 - val_dice_coef: 0.9555\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.04444\n",
      "Epoch 78/150\n",
      "320000/320000 [==============================] - 1912s 6ms/step - loss: 0.0399 - dice_coef: 0.9601 - val_loss: 0.0445 - val_dice_coef: 0.9555\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.04444\n",
      "Epoch 79/150\n",
      "320000/320000 [==============================] - 1814s 6ms/step - loss: 0.0399 - dice_coef: 0.9601 - val_loss: 0.0445 - val_dice_coef: 0.9555\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.04444\n",
      "Epoch 80/150\n",
      "320000/320000 [==============================] - 1923s 6ms/step - loss: 0.0398 - dice_coef: 0.9602 - val_loss: 0.0445 - val_dice_coef: 0.9555\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.04444\n",
      "Epoch 81/150\n",
      "320000/320000 [==============================] - 1810s 6ms/step - loss: 0.0398 - dice_coef: 0.9602 - val_loss: 0.0445 - val_dice_coef: 0.9555\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.04444\n",
      "Epoch 82/150\n",
      "320000/320000 [==============================] - 1905s 6ms/step - loss: 0.0398 - dice_coef: 0.9602 - val_loss: 0.0444 - val_dice_coef: 0.9556\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.04444 to 0.04438, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 83/150\n",
      "320000/320000 [==============================] - 1809s 6ms/step - loss: 0.0397 - dice_coef: 0.9603 - val_loss: 0.0446 - val_dice_coef: 0.9554\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.04438\n",
      "Epoch 84/150\n",
      "320000/320000 [==============================] - 1920s 6ms/step - loss: 0.0397 - dice_coef: 0.9603 - val_loss: 0.0443 - val_dice_coef: 0.9557\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.04438 to 0.04429, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 85/150\n",
      "320000/320000 [==============================] - 1808s 6ms/step - loss: 0.0397 - dice_coef: 0.9603 - val_loss: 0.0444 - val_dice_coef: 0.9556\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.04429\n",
      "Epoch 86/150\n",
      "320000/320000 [==============================] - 1897s 6ms/step - loss: 0.0396 - dice_coef: 0.9604 - val_loss: 0.0446 - val_dice_coef: 0.9554\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.04429\n",
      "Epoch 87/150\n",
      "320000/320000 [==============================] - 1807s 6ms/step - loss: 0.0396 - dice_coef: 0.9604 - val_loss: 0.0444 - val_dice_coef: 0.9556\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.04429\n",
      "Epoch 88/150\n",
      "320000/320000 [==============================] - 1948s 6ms/step - loss: 0.0396 - dice_coef: 0.9604 - val_loss: 0.0443 - val_dice_coef: 0.9557\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.04429\n",
      "Epoch 89/150\n",
      "320000/320000 [==============================] - 1832s 6ms/step - loss: 0.0395 - dice_coef: 0.9605 - val_loss: 0.0445 - val_dice_coef: 0.9555\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.04429\n",
      "Epoch 90/150\n",
      "320000/320000 [==============================] - 1906s 6ms/step - loss: 0.0395 - dice_coef: 0.9605 - val_loss: 0.0444 - val_dice_coef: 0.9556\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.04429\n",
      "Epoch 91/150\n",
      "320000/320000 [==============================] - 1816s 6ms/step - loss: 0.0395 - dice_coef: 0.9605 - val_loss: 0.0444 - val_dice_coef: 0.9556\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.04429\n",
      "Epoch 92/150\n",
      "320000/320000 [==============================] - 1927s 6ms/step - loss: 0.0395 - dice_coef: 0.9605 - val_loss: 0.0444 - val_dice_coef: 0.9556\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.04429\n",
      "Epoch 93/150\n",
      "320000/320000 [==============================] - 1825s 6ms/step - loss: 0.0394 - dice_coef: 0.9606 - val_loss: 0.0444 - val_dice_coef: 0.9556\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.04429\n",
      "Epoch 94/150\n",
      "320000/320000 [==============================] - 1916s 6ms/step - loss: 0.0394 - dice_coef: 0.9606 - val_loss: 0.0444 - val_dice_coef: 0.9556\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.04429\n",
      "Epoch 95/150\n",
      "320000/320000 [==============================] - 1813s 6ms/step - loss: 0.0394 - dice_coef: 0.9606 - val_loss: 0.0445 - val_dice_coef: 0.9555\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.04429\n",
      "Epoch 96/150\n",
      "320000/320000 [==============================] - 1914s 6ms/step - loss: 0.0393 - dice_coef: 0.9607 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.04429 to 0.04424, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 97/150\n",
      "320000/320000 [==============================] - 1826s 6ms/step - loss: 0.0393 - dice_coef: 0.9607 - val_loss: 0.0441 - val_dice_coef: 0.9559\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.04424 to 0.04413, saving model to ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/best_weights.h5\n",
      "Epoch 98/150\n",
      "320000/320000 [==============================] - 1936s 6ms/step - loss: 0.0393 - dice_coef: 0.9607 - val_loss: 0.0443 - val_dice_coef: 0.9557\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.04413\n",
      "Epoch 99/150\n",
      "320000/320000 [==============================] - 1808s 6ms/step - loss: 0.0393 - dice_coef: 0.9607 - val_loss: 0.0443 - val_dice_coef: 0.9557\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.04413\n",
      "Epoch 100/150\n",
      "320000/320000 [==============================] - 1911s 6ms/step - loss: 0.0392 - dice_coef: 0.9608 - val_loss: 0.0444 - val_dice_coef: 0.9556\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.04413\n",
      "Epoch 101/150\n",
      "320000/320000 [==============================] - 1818s 6ms/step - loss: 0.0392 - dice_coef: 0.9608 - val_loss: 0.0443 - val_dice_coef: 0.9557\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.04413\n",
      "Epoch 102/150\n",
      "320000/320000 [==============================] - 1920s 6ms/step - loss: 0.0392 - dice_coef: 0.9608 - val_loss: 0.0441 - val_dice_coef: 0.9559\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.04413\n",
      "Epoch 103/150\n",
      "320000/320000 [==============================] - 1822s 6ms/step - loss: 0.0392 - dice_coef: 0.9608 - val_loss: 0.0443 - val_dice_coef: 0.9557\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.04413\n",
      "Epoch 104/150\n",
      "320000/320000 [==============================] - 1913s 6ms/step - loss: 0.0391 - dice_coef: 0.9609 - val_loss: 0.0443 - val_dice_coef: 0.9557\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.04413\n",
      "Epoch 105/150\n",
      "320000/320000 [==============================] - 1821s 6ms/step - loss: 0.0391 - dice_coef: 0.9609 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.04413\n",
      "Epoch 106/150\n",
      "320000/320000 [==============================] - 1909s 6ms/step - loss: 0.0391 - dice_coef: 0.9609 - val_loss: 0.0443 - val_dice_coef: 0.9557\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.04413\n",
      "Epoch 107/150\n",
      "320000/320000 [==============================] - 1805s 6ms/step - loss: 0.0391 - dice_coef: 0.9609 - val_loss: 0.0444 - val_dice_coef: 0.9556\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.04413\n",
      "Epoch 108/150\n",
      "320000/320000 [==============================] - 1912s 6ms/step - loss: 0.0390 - dice_coef: 0.9610 - val_loss: 0.0443 - val_dice_coef: 0.9557\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.04413\n",
      "Epoch 109/150\n",
      "320000/320000 [==============================] - 1808s 6ms/step - loss: 0.0390 - dice_coef: 0.9610 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.04413\n",
      "Epoch 110/150\n",
      "320000/320000 [==============================] - 1922s 6ms/step - loss: 0.0390 - dice_coef: 0.9610 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.04413\n",
      "Epoch 111/150\n",
      "320000/320000 [==============================] - 1814s 6ms/step - loss: 0.0390 - dice_coef: 0.9610 - val_loss: 0.0443 - val_dice_coef: 0.9557\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.04413\n",
      "Epoch 112/150\n",
      "320000/320000 [==============================] - 1909s 6ms/step - loss: 0.0389 - dice_coef: 0.9611 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.04413\n",
      "\n",
      "Epoch 00112: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "Epoch 113/150\n",
      "320000/320000 [==============================] - 1818s 6ms/step - loss: 0.0389 - dice_coef: 0.9611 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.04413\n",
      "Epoch 114/150\n",
      "320000/320000 [==============================] - 1928s 6ms/step - loss: 0.0389 - dice_coef: 0.9611 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.04413\n",
      "Epoch 115/150\n",
      "320000/320000 [==============================] - 1820s 6ms/step - loss: 0.0389 - dice_coef: 0.9611 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.04413\n",
      "Epoch 116/150\n",
      "320000/320000 [==============================] - 1907s 6ms/step - loss: 0.0389 - dice_coef: 0.9611 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.04413\n",
      "Epoch 117/150\n",
      "320000/320000 [==============================] - 1810s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.04413\n",
      "Epoch 118/150\n",
      "320000/320000 [==============================] - 1936s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.04413\n",
      "Epoch 119/150\n",
      "320000/320000 [==============================] - 1818s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.04413\n",
      "Epoch 120/150\n",
      "320000/320000 [==============================] - 1914s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.04413\n",
      "Epoch 121/150\n",
      "320000/320000 [==============================] - 1822s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.04413\n",
      "Epoch 122/150\n",
      "320000/320000 [==============================] - 1915s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.04413\n",
      "Epoch 123/150\n",
      "320000/320000 [==============================] - 1784s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.04413\n",
      "Epoch 124/150\n",
      "320000/320000 [==============================] - 1922s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.04413\n",
      "Epoch 125/150\n",
      "320000/320000 [==============================] - 1822s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.04413\n",
      "Epoch 126/150\n",
      "320000/320000 [==============================] - 1905s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.04413\n",
      "Epoch 127/150\n",
      "320000/320000 [==============================] - 1792s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.04413\n",
      "\n",
      "Epoch 00127: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "Epoch 128/150\n",
      "320000/320000 [==============================] - 1918s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.04413\n",
      "Epoch 129/150\n",
      "320000/320000 [==============================] - 1819s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.04413\n",
      "Epoch 130/150\n",
      "320000/320000 [==============================] - 1908s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.04413\n",
      "Epoch 131/150\n",
      "320000/320000 [==============================] - 1816s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.04413\n",
      "Epoch 132/150\n",
      "320000/320000 [==============================] - 1914s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.04413\n",
      "Epoch 133/150\n",
      "320000/320000 [==============================] - 1816s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.04413\n",
      "Epoch 134/150\n",
      "320000/320000 [==============================] - 1913s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.04413\n",
      "Epoch 135/150\n",
      "320000/320000 [==============================] - 1817s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.04413\n",
      "Epoch 136/150\n",
      "320000/320000 [==============================] - 1919s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.04413\n",
      "Epoch 137/150\n",
      "320000/320000 [==============================] - 1817s 6ms/step - loss: 0.0388 - dice_coef: 0.9612 - val_loss: 0.0442 - val_dice_coef: 0.9558\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.04413\n",
      "Epoch 00137: early stopping\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Keras provides a set of functions called callbacks: \n",
    "you can think of callbacks as events that will be triggered at certain training states. \n",
    "The callback we need for checkpointing is the ModelCheckpoint \n",
    "which provides all the features we need according to the checkpointing strategy we adopted in our example\n",
    "'''\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./'+save_folder+'/'+name_experiment+'/best_weights.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True) #save at each epoch if the validation decreased\n",
    "\n",
    "print('[training session] before mask unet func patch mask shape : ',patches_masks_train.shape)\n",
    "patches_masks_train = masks_Unet(patches_masks_train)  #reduce memory consumption\n",
    "\n",
    "#patches_masks_train = masks_Unet(temp_label)\n",
    "print('[training session] after mask unet func patch mask shape : ',patches_masks_train.shape)\n",
    "\n",
    "#history = model.fit(temp_img, temp_label, epochs=num_epochs, batch_size=batch_size, verbose=1, shuffle=True, validation_split=0.2, callbacks=[checkpointer])\n",
    "\n",
    "\n",
    "history = model.fit(patches_imgs_train, patches_masks_train, epochs=num_epochs, batch_size=batch_size, verbose=1, shuffle=True, validation_split=0.2, callbacks=[checkpointer, reduce_lr , early_stopping])\n",
    "model.save_weights('./'+save_folder+'/'+name_experiment +'/last_weights.h5', overwrite=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist the folder in this path : ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/train_history\n",
      "already exist the folder in this path : ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/train_history\n",
      "already exist the folder in this path : ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/train_history\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss= history.history['val_loss']\n",
    "acc = history.history['dice_coef']\n",
    "val_acc = history.history['val_dice_coef']\n",
    "\n",
    "epochs = range(1,len(acc) +1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label = \"Training loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label = \"Validation loss\")\n",
    "plt.title(\"Training and Validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.savefig('./'+save_folder+'/'+name_experiment+\"/training_loss_result.png\")\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'ro', label = \"Training dice coef\")\n",
    "plt.plot(epochs, val_acc, 'r', label = \"Validation dice coef\")\n",
    "plt.title(\"Training and Validation dice coef\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig('./'+save_folder+'/'+name_experiment+\"/training_acc_result.png\")\n",
    "plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "file_path = './'+save_folder+'/'+name_experiment + '/' + 'train_history'\n",
    "\n",
    "def save_history_txt_csv(history, file_path, file_name):\n",
    "    if os.path.isdir(file_path) == False:\n",
    "        os.mkdir(file_path)\n",
    "    else:\n",
    "        print('already exist the folder in this path : {}'.format(file_path))\n",
    "    \n",
    "    hist_df = pd.DataFrame(history) \n",
    "\n",
    "    # save to json:  \n",
    "    hist_json_file = file_path + '/' + file_name +'.json' \n",
    "    with open(hist_json_file, mode='w') as f:\n",
    "        hist_df.to_json(f)\n",
    "\n",
    "    # or save to csv: \n",
    "    hist_csv_file = file_path + '/' + file_name + '.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "        \n",
    "\n",
    "save_history_txt_csv(loss, file_path, 'train_loss')\n",
    "save_history_txt_csv(val_loss, file_path, 'val_loss')\n",
    "save_history_txt_csv(acc, file_path, 'train_acc')\n",
    "save_history_txt_csv(val_acc, file_path, 'val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist the folder in this path : ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/train_history\n",
      "already exist the folder in this path : ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/train_history\n",
      "already exist the folder in this path : ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/train_history\n",
      "already exist the folder in this path : ./result/figure5/figure5_aug400000_batch16_lr_conjHrf/train_history\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = './'+save_folder+'/'+name_experiment + '/' + 'train_history'\n",
    "\n",
    "def save_history_txt_csv(history, file_path, file_name):\n",
    "    if os.path.isdir(file_path) == False:\n",
    "        os.mkdir(file_path)\n",
    "    else:\n",
    "        print('already exist the folder in this path : {}'.format(file_path))\n",
    "    \n",
    "    hist_df = pd.DataFrame(history) \n",
    "\n",
    "    # save to json:  \n",
    "    hist_json_file = file_path + '/' + file_name +'.json' \n",
    "    with open(hist_json_file, mode='w') as f:\n",
    "        hist_df.to_json(f)\n",
    "\n",
    "    # or save to csv: \n",
    "    hist_csv_file = file_path + '/' + file_name + '.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "        \n",
    "\n",
    "save_history_txt_csv(loss, file_path, 'train_loss')\n",
    "save_history_txt_csv(val_loss, file_path, 'val_loss')\n",
    "save_history_txt_csv(acc, file_path, 'train_acc')\n",
    "save_history_txt_csv(val_acc, file_path, 'val_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
