{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "## Python\n",
    "#import module for predict\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import configparser\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "#Keras\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Model\n",
    "import keras.backend\n",
    "#scikit learn\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import f1_score\n",
    "import sys\n",
    "sys.path.insert(0, './lib/')\n",
    "# help_functions.py\n",
    "from help_functions import *\n",
    "# extract_patches.py\n",
    "from extract_patches import *\n",
    "# pre_processing.py\n",
    "from pre_processing import *\n",
    "\n",
    "from keras.models import Model\n",
    "from multi_scale_line import *\n",
    "\n",
    "\n",
    "RESIZE_CONSTANT = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "def show_on_jupyter(img,color= None,title=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import cv2\n",
    "    \"\"\"Show img on jupyter notebook. No return Value\n",
    "    \n",
    "    You should check the img's color space.\n",
    "    I just consider about RGB color space & 1 ch color space(like green ch, gray space, ...)\n",
    "    \n",
    "    using matplotlib\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : 2-D Array\n",
    "        numpy 2-D array\n",
    "        opencv / sklearn / plt are avaliable.\n",
    "        float / uint8 data type.\n",
    "        \n",
    "    color : string\n",
    "        'gray' or 'None'\n",
    "        'gray' means that img has a 1 ch.\n",
    "        'None' means that img has a RGB ch.\n",
    "        (default: None)\n",
    "        \n",
    "    title : string\n",
    "        decide img's title\n",
    "        (default : None)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        No return value.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> img = cv2.imread(img_path)\n",
    "    >>> show_on_jupyter(img)\n",
    "    \n",
    "    img has a 1 ch\n",
    "    >>> img = cv2.imread(img_path)\n",
    "    >>> show_on_jupyter(img,'gray')\n",
    "    \"\"\"\n",
    "    if color == 'gray':\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(img,cmap=color)\n",
    "        plt.show()\n",
    "    elif color == None:\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Gray or None\")\n",
    "\n",
    "\n",
    "def load_data(data_path):\n",
    "    \n",
    "    imgs = np.empty((NUM_IMG,1,HEIGHT_IMG,WIDTH_IMG))\n",
    "    cnt = 0\n",
    "    for path, subdirs, files in os.walk(data_path): #list all files, directories in the path\n",
    "        for i in range(len(files)):\n",
    "            tempImg = Image.open(data_path + files[i])\n",
    "            #print(files[i])\n",
    "            #tempImg = np.asarray(tempImg)\n",
    "            #print(tempImg.shape)\n",
    "            w, h = tempImg.size\n",
    "            print(w,h)\n",
    "            print('w,h : ', w,'x',h)\n",
    "            if (w == 700 and h ==380):\n",
    "                print(\"Hi\")\n",
    "                tempImg = np.asarray(tempImg)\n",
    "                tempImg = tempImg[np.newaxis,:,:]\n",
    "                imgs[cnt] = tempImg\n",
    "                cnt += 1\n",
    "            \n",
    "    return imgs\n",
    "\n",
    "def group_plot(data,row,col):\n",
    "    fig=plt.figure(figsize=(13, 13))\n",
    "    columns = col\n",
    "    rows = row\n",
    "    for num in range(len(data)):\n",
    "        img = data[num]\n",
    "        fig.add_subplot(rows, columns, num+1)\n",
    "        plt.imshow(img,cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "def get_conjunctivaData_testing(Conjunctiva_test_img_ori, num_test_img,patch_h, patch_w):\n",
    "    \n",
    "    # get img data\n",
    "    test_img_ori = Conjunctiva_test_img_ori\n",
    "    print(np.shape(test_img_ori))\n",
    "    test_imgs = my_gray_preprocessing(test_img_ori)\n",
    "    print(np.shape(test_img_ori))\n",
    "    # extend both images and masks so they can be divided exactly by the patches dimensions\n",
    "    # make tensor data.\n",
    "    \n",
    "    test_imgs = test_imgs[0:num_test_img,:,:,:]\n",
    "    print('type : ',type(test_imgs))\n",
    "    print('[get data testing func] prev test img shape : {} '.format(test_imgs.shape))\n",
    "    test_imgs = paint_border(test_imgs,patch_h, patch_w)\n",
    "    print('[get data testing func] after test img shape : {} '.format(test_imgs.shape))\n",
    "\n",
    "    #data_consistency_check(test_imgs, test_grds)\n",
    "    #assert(np.max(test_grds)==1  and np.min(test_grds)==0)\n",
    "    \n",
    "    print (\"[get_data_testing_func] test images range (min-max): \" +str(np.min(test_imgs)) +' - '+str(np.max(test_imgs)))\n",
    "    print (\"[get_data_testing_fucn] test masks are within 0-1\\n\")\n",
    "    \n",
    "    patches_imgs_test = extract_ordered(test_imgs,patch_h,patch_w)\n",
    "    print (\"\\n[get_data_testing_fucn] test PATCHES images/grds shape:\")\n",
    "    print (patches_imgs_test.shape)\n",
    "    print (\"[get_data_testing_fucn] test PATCHES images range (min-max): {} - {}\".format(str(np.min(patches_imgs_test)), str(np.max(patches_imgs_test))))\n",
    "    \n",
    "    return patches_imgs_test\n",
    "    \n",
    "    \n",
    "def width_padding(img, pad_size):\n",
    "    h,w = np.shape(img)\n",
    "    #print('pad size', pad_size)\n",
    "    \n",
    "    if pad_size % 2 ==1: #odd number\n",
    "        pad01 = np.zeros((h, pad_size//2))\n",
    "        pad02 = np.zeros((h, pad_size//2 +1))\n",
    "    \n",
    "        result = np.hstack([pad01, img])\n",
    "        result = np.hstack([result, pad02])\n",
    "        \n",
    "    else:\n",
    "        pad01 = np.zeros((h, pad_size//2))\n",
    "    \n",
    "        result = np.hstack([pad01, img])\n",
    "        result = np.hstack([result, pad01])\n",
    "        \n",
    "    print(np.shape(img), ' ',np.shape(result))\n",
    "    return result\n",
    "    \n",
    "def height_padding(img,pad_size):\n",
    "    h,w = np.shape(img)\n",
    "    #print('pad size : ',pad_size)\n",
    "    \n",
    "    if pad_size % 2 ==1: #odd number\n",
    "        pad01 = np.zeros((pad_size//2,w ))\n",
    "        pad02 = np.zeros((pad_size//2 +1 , w))\n",
    "        \n",
    "        result = np.vstack([pad01, img])\n",
    "        result = np.vstack([result,pad02])\n",
    "        \n",
    "    else:\n",
    "        pad01 = np.zeros((pad_size//2, w))\n",
    "    \n",
    "        result = np.vstack([pad01, img])\n",
    "        result = np.vstack([result, pad01])\n",
    "    print(np.shape(img), ' ',np.shape(result))\n",
    "    return result\n",
    "\n",
    "    patches_imgs_test = extract_ordered(test_imgs,patch_h,patch_w)\n",
    "    #data_consistency_check(test_imgs, test_grds)\n",
    "\n",
    "    print (\"\\n[get_data_testing_fucn] test PATCHES images/grds shape:\")\n",
    "    print (patches_imgs_test.shape)\n",
    "    print (\"[get_data_testing_fucn] test PATCHES images range (min-max): {} - {}\".format(str(np.min(patches_imgs_test)), str(np.max(patches_imgs_test))))\n",
    "\n",
    "    return patches_imgs_test\n",
    "\n",
    "def get_conjunctivaData_testing_overlap(Conjunctiva_test_img_ori, num_test_img,\n",
    "                                        patch_h,patch_w,stride_h, stride_w):\n",
    "    \n",
    "    # get img data\n",
    "    test_img_ori = Conjunctiva_test_img_ori\n",
    "    print(np.shape(test_img_ori))\n",
    "    test_imgs = my_gray_preprocessing(test_img_ori)\n",
    "    #test_imgs = test_imgs/255.0\n",
    "    \n",
    "    # extend both images and masks so they can be divided exactly by the patches dimensions\n",
    "    # make tensor data.\n",
    "    \n",
    "    test_imgs = test_imgs[0:num_test_img,:,:,:]\n",
    "    \n",
    "    #print('type : ',type(test_imgs))\n",
    "    #print('[get data testing overlap] prev test img shape : {}  '.format(test_imgs.shape))\n",
    "    test_imgs = paint_border_overlap(test_imgs,patch_h, patch_w,stride_h, stride_w)\n",
    "    #test_grds = paint_border_overlap(test_grds, patch_h, patch_w,stride_h, stride_w)\n",
    "    #print('[get data testing overlap] after test img shape : {}'.format(test_imgs.shape))\n",
    "    \n",
    "    print (\"[get_data_testing_overlap func] test images range (min-max): \" +str(np.min(test_imgs)) +' - '+str(np.max(test_imgs)))\n",
    "    #print (\"[get_data_testing_overlap fucn] test masks are within 0-1\\n\")\n",
    "    \n",
    "    patches_imgs_test = extract_ordered_overlap(test_imgs,patch_h,patch_w,stride_h,stride_w)\n",
    "    \n",
    "    #print (\"\\n[get_data_testing_overlap func] test PATCHES images shape:\")\n",
    "    print (patches_imgs_test.shape)\n",
    "    #print (\"[get_data_testing_overlap func] test PATCHES images range (min-max): \" +str(np.min(patches_imgs_test)) +' - '+str(np.max(patches_imgs_test)))\n",
    "\n",
    "    return patches_imgs_test, test_imgs.shape[2], test_imgs.shape[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print(584 /64)\n",
    "print(565/64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여기부터 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./result/201130/14_jmh/\n",
      "./result/figure5/figure5_aug400000_batch16_lr_conjHrf/\n",
      "201130/14_jmh/\n"
     ]
    }
   ],
   "source": [
    "#========= CONFIG FILE TO READ FROM =======\n",
    "config = configparser.RawConfigParser()\n",
    "config.read('conj_configuration.txt')\n",
    "\n",
    "#run the training on invariant or local\n",
    "path_data = config.get('test data path', 'conj_test_data')\n",
    "path_ground_truth = config.get('data paths', 'conj_path_ground')\n",
    "\n",
    "patch_height = int(config.get('data attributes', 'patch_height'))\n",
    "patch_width = int(config.get('data attributes', 'patch_width'))\n",
    "\n",
    "stride_height = int(config.get('testing settings', 'stride_height'))\n",
    "stride_width = int(config.get('testing settings', 'stride_width'))\n",
    "\n",
    "name_experiment = config.get('experiment name', 'name')\n",
    "\n",
    "path_experiment = './' +'result/'+name_experiment\n",
    "average_mode = config.getboolean('testing settings', 'average_mode')\n",
    "\n",
    "# retina \n",
    "target_experiment = config.get('testing settings','experiment_target')\n",
    "\n",
    "pretrain_path = config.get('fine tuning','pretrain path')\n",
    "\n",
    "if config.get('fine tuning','fine_tuning') ==True:\n",
    "    pretrained_dir = config.get('fine tuning', 'pretrained_dir')\n",
    "\n",
    "\n",
    "if os.path.isdir(path_experiment) == False:\n",
    "    os.mkdir(path_experiment)\n",
    "else:\n",
    "    print('already exist the folder in this path : {}'.format(path_experiment))\n",
    "\n",
    "    \n",
    "print(path_experiment)\n",
    "print(pretrain_path)\n",
    "print(name_experiment)\n",
    "#print(target_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/201130/proc_data/14_jmh/\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "filst = sorted(glob.glob('./'+name_experiment))\n",
    "print(path_data)\n",
    "print(patch_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/201130/proc_data/14_jmh/ 00.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 01.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 02.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 03.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 04.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 05.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 06.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 07.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 08.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 09.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 10.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 11.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 12.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 13.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 14.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 15.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 16.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 17.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 18.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 19.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 20.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 21.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 22.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 23.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 24.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 25.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 26.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 27.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 28.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 29.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 30.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 31.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 32.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 33.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 34.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "./data/201130/proc_data/14_jmh/ 35.png\n",
      "(1848, 2208)\n",
      "(924, 1104)\n",
      "(36, 924, 1104)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# file order test\n",
    "\n",
    "#case 01 : 2208_1848\n",
    "#temp = cv2.imread('./data/20_04_16_gHos/2208_1848/5_imgs/sub_50_119_reg/00.png')\n",
    "\n",
    "\n",
    "#print(np.shape(temp))\n",
    "#temp = np.array((2208, 1848))\n",
    "RESIZE_CONSTANT = 0.5\n",
    "HEIGHT_IMG = int(1848 * RESIZE_CONSTANT) #(2054, 2456)\n",
    "WIDTH_IMG = int(2208 *RESIZE_CONSTANT)\n",
    "NUM_IMG = 0\n",
    "CONJ_IMG_LIST = []\n",
    "\n",
    "for count, filename in enumerate(sorted(os.listdir(path_data)), start=1):\n",
    "    if filename.startswith(\".ipynb\")==False:\n",
    "        #print('Enter {} to select {}'.format(count, filename))\n",
    "        print(path_data,filename)\n",
    "        tempImg = cv2.imread(path_data + filename,0)\n",
    "        print(np.shape(tempImg))\n",
    "        tempImg = cv2.resize(tempImg, (WIDTH_IMG, HEIGHT_IMG), interpolation = cv2.INTER_CUBIC)\n",
    "        print(np.shape(tempImg))\n",
    "        CONJ_IMG_LIST.append(tempImg)\n",
    "\n",
    "        NUM_IMG = NUM_IMG +1\n",
    "    \n",
    "CONJ_IMG_LIST = np.asarray(CONJ_IMG_LIST)\n",
    "print(np.shape(CONJ_IMG_LIST))\n",
    "print(type(CONJ_IMG_LIST))\n",
    "\n",
    "if os.path.isdir(path_experiment+str(RESIZE_CONSTANT)+'_test_imgs/') == False:\n",
    "    os.mkdir(path_experiment+str(RESIZE_CONSTANT)+'_test_imgs/')\n",
    "else:\n",
    "    print('already exist the folder in this path : {}'.format(path_experiment+str(RESIZE_CONSTANT)+'_test_imgs/'))\n",
    "\n",
    "for i in range(np.shape(CONJ_IMG_LIST)[0]):\n",
    "    if i > 9:\n",
    "        cv2.imwrite(path_experiment + str(RESIZE_CONSTANT) + '_test_imgs/'+str(i)+'.png',CONJ_IMG_LIST[i])\n",
    "    else:\n",
    "        cv2.imwrite(path_experiment +str(RESIZE_CONSTANT)+ '_test_imgs/0'+str(i)+'.png',CONJ_IMG_LIST[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist the folder in this path : ./result/201130/14_jmh/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./result/201130/14_jmh/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isdir(path_experiment) == False:\n",
    "    os.mkdir(path_experiment)\n",
    "else:\n",
    "    print('already exist the folder in this path : {}'.format(path_experiment))\n",
    "path_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 1, 924, 1104)\n",
      "[DEBUG preprocessing] :  (36, 1, 924, 1104)\n",
      "new full images shape: \n",
      "(36, 1, 924, 1104)\n",
      "[get_data_testing_overlap func] test images range (min-max): 0.0 - 1.0\n",
      "extract , :  860 20 1040 20\n",
      "[extrct_order_overlap func] Number of patches on h : 44\n",
      "[extrct_order_overlap func] Number of patches on w : 53\n",
      "[extrct_order_overlap func] number of patches per image: 2332 totally for this dataset: 83952\n",
      "(83952, 1, 64, 64)\n",
      "[group images func] prev data shape  : (100, 1, 64, 64)\n",
      "[group images func] after data shape :  (100, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 128, 1)\n",
      "[group images func] final total image :  (3264, 128, 1)\n",
      "data shape :  (3264, 128, 1)\n",
      "<PIL.Image.Image image mode=L size=128x3264 at 0x7F3EDC8A9FD0>\n",
      "file name :  ./result/201130/14_jmh/conj_overlap_test_patch_img\n"
     ]
    }
   ],
   "source": [
    "#conj_img = load_data(path_data) \n",
    "conj_img = np.expand_dims(CONJ_IMG_LIST,axis =1)\n",
    "\n",
    "if average_mode == True:\n",
    "    patches_conj_imgs_test, new_height, new_width = get_conjunctivaData_testing_overlap(conj_img,\n",
    "                                                         NUM_IMG,\n",
    "                                                         patch_height,\n",
    "                                                         patch_width,\n",
    "                                                         stride_height,\n",
    "                                                        stride_width)\n",
    "    conj_totimg = visualize(group_images(patches_conj_imgs_test[100:200,:,:,:]*255,2),path_experiment+'conj_overlap_test_patch_img')\n",
    "else:\n",
    "    patches_conj_imgs_test = get_conjunctivaData_testing(conj_img,\n",
    "                                                         NUM_IMG,\n",
    "                                                         patch_height,\n",
    "                                                         patch_width)\n",
    "    \n",
    "    print(np.shape(patches_conj_imgs_test))\n",
    "    conj_totimg = visualize(group_images(patches_conj_imgs_test[100:200,:,:,:]*255,2),path_experiment+'conj_test_patch_img')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#================ Run the prediction of the patches ==================================\n",
    "best_last = config.get('testing settings', 'best_last')\n",
    "#Load the saved model\n",
    "\n",
    "inference_mode = config.get('testing settings', 'mode')\n",
    "if inference_mode == 'conj':\n",
    "    model = model_from_json(open(path_experiment +'_architecture.json').read())\n",
    "    model.load_weights(path_experiment +best_last+'_weights.h5')\n",
    "    \n",
    "elif inference_mode == 'retina':\n",
    "    model = model_from_json(open(pretrain_path +target_experiment+'_architecture.json').read())\n",
    "    model.load_weights(pretrain_path +best_last+'_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83952/83952 [==============================] - 56s 670us/step\n"
     ]
    }
   ],
   "source": [
    "conj_predictions = model.predict(patches_conj_imgs_test, batch_size=32, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[group images func] prev data shape  : (50, 1, 64, 64)\n",
      "[group images func] after data shape :  (50, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 128, 1)\n",
      "[group images func] final total image :  (1664, 128, 1)\n",
      "data shape :  (1664, 128, 1)\n",
      "<PIL.Image.Image image mode=L size=128x1664 at 0x7F3EDC8A9198>\n",
      "file name :  ./result/201130/14_jmh//predict_conj_patch_ori\n",
      "[group images func] prev data shape  : (50, 1, 64, 64)\n",
      "[group images func] after data shape :  (50, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 128, 1)\n",
      "[group images func] final total image :  (1664, 128, 1)\n",
      "data shape :  (1664, 128, 1)\n",
      "<PIL.Image.Image image mode=L size=128x1664 at 0x7F3EDC8A95F8>\n",
      "file name :  ./result/201130/14_jmh//predict_conj_patch_thresh\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x1664 at 0x7F3EDC8A95F8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_conj_patches = pred_to_imgs(conj_predictions, patch_height, patch_width, \"original\") \n",
    "pred_conj_patches_thresh = pred_to_imgs(conj_predictions, patch_height, patch_width, \"threshold\") \n",
    "\n",
    "\n",
    "visualize(group_images(pred_conj_patches[150:200,:,:,:],2),path_experiment+'/predict_conj_patch_ori')\n",
    "visualize(group_images(pred_conj_patches_thresh[150:200,:,:,:],2),path_experiment+'/predict_conj_patch_thresh')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_patches_h: 44\n",
      "N_patches_w: 53\n",
      "N_patches_img: 2332\n",
      "According to the dimension inserted, there are 36 full images (of 924x1104 each)\n",
      "(36, 1, 924, 1104)\n",
      "N_patches_h: 44\n",
      "N_patches_w: 53\n",
      "N_patches_img: 2332\n",
      "According to the dimension inserted, there are 36 full images (of 924x1104 each)\n",
      "(36, 1, 924, 1104)\n",
      "N_patches_h: 44\n",
      "N_patches_w: 53\n",
      "N_patches_img: 2332\n",
      "According to the dimension inserted, there are 36 full images (of 924x1104 each)\n",
      "(36, 1, 924, 1104)\n",
      "[group images func] prev data shape  : (36, 1, 924, 1104)\n",
      "[group images func] after data shape :  (36, 924, 1104, 1)\n",
      "[group images func] first total image :  (924, 1104, 1)\n",
      "[group images func] final total image :  (34188, 1104, 1)\n",
      "data shape :  (34188, 1104, 1)\n",
      "<PIL.Image.Image image mode=L size=1104x34188 at 0x7F3DF1EABDD8>\n",
      "file name :  ./result/201130/14_jmh//conj_predict_imgs\n",
      "[group images func] prev data shape  : (36, 1, 924, 1104)\n",
      "[group images func] after data shape :  (36, 924, 1104, 1)\n",
      "[group images func] first total image :  (924, 1104, 1)\n",
      "[group images func] final total image :  (34188, 1104, 1)\n",
      "data shape :  (34188, 1104, 1)\n",
      "<PIL.Image.Image image mode=L size=1104x34188 at 0x7F3DF1EAB748>\n",
      "file name :  ./result/201130/14_jmh//conj_predict_thresh_imgs\n",
      "[group images func] prev data shape  : (36, 1, 924, 1104)\n",
      "[group images func] after data shape :  (36, 924, 1104, 1)\n",
      "[group images func] first total image :  (924, 1104, 1)\n",
      "[group images func] final total image :  (34188, 1104, 1)\n",
      "data shape :  (34188, 1104, 1)\n",
      "<PIL.Image.Image image mode=L size=1104x34188 at 0x7F3DF1EAB860>\n",
      "file name :  ./result/201130/14_jmh//conj_ori_imgs\n"
     ]
    }
   ],
   "source": [
    "if average_mode == True:\n",
    "    conj_pred_imgs = recompose_overlap_img(pred_conj_patches, new_height, new_width, stride_height, stride_width)# predictions\n",
    "    conj_pred_imgs_thr = recompose_overlap_img(pred_conj_patches_thresh, new_height, new_width, stride_height, stride_width)# predictions\n",
    "    conj_ori_imgs = recompose_overlap_img(patches_conj_imgs_test,new_height, new_width, stride_height, stride_width)\n",
    "   \n",
    "    visualize(group_images(conj_pred_imgs[:,:,:,:],1),path_experiment+'/conj_predict_imgs')\n",
    "    visualize(group_images(conj_pred_imgs_thr[:,:,:,:],1),path_experiment+'/conj_predict_thresh_imgs')\n",
    "    visualize(group_images(conj_ori_imgs[:,:,:,:],1), path_experiment+'/conj_ori_imgs')\n",
    "\n",
    "\n",
    "else:\n",
    "    conj_pred_imgs = recompose_img(pred_conj_patches,33,39)\n",
    "    conj_pred_imgs_thresh = recompose_img(pred_conj_patches_thresh,33,39)\n",
    "    conj_ori_imgs = recompose_img(patches_conj_imgs_test,33,39)\n",
    "\n",
    "    visualize(group_images(conj_pred_imgs[:,:,:,:],2),path_experiment+'/conj_predict_imgs')\n",
    "    visualize(group_images(conj_pred_imgs_thresh[:,:,:,:],2),path_experiment+'/conj_predict_thresh_imgs')\n",
    "    visualize(group_images(conj_ori_imgs[:,:,:,:],2), path_experiment+'/conj_ori_imgs')\n",
    "\n",
    "full_img_height = conj_img.shape[2]\n",
    "full_img_width = conj_img.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 1, 924, 1104)\n"
     ]
    }
   ],
   "source": [
    "# make each prediction images\n",
    "\n",
    "\n",
    "if average_mode == True:\n",
    "    conj_pred_imgs = conj_pred_imgs[:,:,0:full_img_height,0:full_img_width]\n",
    "    conj_pred_imgs_thr = conj_pred_imgs_thr[:,:,0:full_img_height,0:full_img_width]\n",
    "    #conj_ori_imgs = gtruth_masks[:,:,0:full_img_height,0:full_img_width]\n",
    "    \n",
    "    print(np.shape(conj_pred_imgs_thr))\n",
    "\n",
    "    if os.path.isdir(path_experiment + 'predicted_imgs') == False:\n",
    "        os.mkdir(path_experiment + 'predicted_imgs')\n",
    "    else:\n",
    "        print('already exist the folder in this path : {}'.format(path_experiment + 'predicted_imgs'))\n",
    "\n",
    "    temp_predicted_imgs = None\n",
    "\n",
    "    for i in range(len(conj_pred_imgs_thr)):\n",
    "        temp_predicted_imgs = conj_pred_imgs_thr[i,0,:,:] * 255\n",
    "        temp_save_imgs = Image.fromarray(temp_predicted_imgs.astype(np.uint8))\n",
    "        if i < 10:\n",
    "            temp_save_imgs.save(path_experiment+'predicted_imgs/0'+str(i)+'_predict.png')\n",
    "        else:\n",
    "            temp_save_imgs.save(path_experiment+'predicted_imgs/'+str(i)+'_predict.png')\n",
    "\n",
    "\n",
    "else:\n",
    "    conj_pred_imgs = conj_pred_imgs[:,:,0:full_img_height,0:full_img_width]\n",
    "    conj_pred_imgs_thresh = conj_pred_imgs_thresh[:,:,0:full_img_height,0:full_img_width]\n",
    "    #conj_ori_imgs = gtruth_masks[:,:,0:full_img_height,0:full_img_width]\n",
    "    \n",
    "    print(np.shape(conj_pred_imgs_thresh))\n",
    "\n",
    "    if os.path.isdir(path_experiment + 'predicted_imgs') == False:\n",
    "        os.mkdir(path_experiment + 'predicted_imgs')\n",
    "    else:\n",
    "        print('already exist the folder in this path : {}'.format(path_experiment + 'predicted_imgs'))\n",
    "\n",
    "    temp_predicted_imgs = None\n",
    "\n",
    "    for i in range(len(conj_pred_imgs_thresh)):\n",
    "        temp_predicted_imgs = conj_pred_imgs_thresh[i,0,:,:] * 255\n",
    "        temp_save_imgs = Image.fromarray(temp_predicted_imgs.astype(np.uint8))\n",
    "        if i <10:\n",
    "            temp_save_imgs.save(path_experiment+'predicted_imgs/0'+str(i)+'_predict.png')\n",
    "        else:\n",
    "            temp_save_imgs.save(path_experiment+'predicted_imgs/'+str(i)+'_predict.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 1, 924, 1104)\n",
      "(36, 1, 924, 1104)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(conj_img))\n",
    "print(np.shape(conj_pred_imgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./result/201130/14_jmh/'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
