{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import configparser\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model as plot\n",
    "from keras.optimizers import SGD\n",
    "from keras import models\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './lib/')\n",
    "from help_functions import *\n",
    "#function to obtain data for training/testing (validation)\n",
    "from extract_patches import get_data_training\n",
    "from model_lib import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model init block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result/conjunctival_result/20_03_20_test01\n"
     ]
    }
   ],
   "source": [
    "config = configparser.RawConfigParser()\n",
    "config.read('conj_configuration.txt')\n",
    "\n",
    "path_imgs = config.get('data paths', 'conj_path_img') # training data\n",
    "path_ground_truth = config.get('data paths', 'conj_path_ground') #training label\n",
    "name_experiment = config.get('experiment name', 'name') # save folder\n",
    "\n",
    "num_epochs = int(config.get('training settings', 'num_epochs')) # train epoch\n",
    "batch_size = int(config.get('training settings', 'batch_size')) # batch size\n",
    "\n",
    "fine_tuning_flag = config.get('fine tuning', 'fine_tuning') # is fine tuning? If you wanr to use retina-pretrain data -> True\n",
    "pretrained_dir = config.get('fine tuning', 'pretrained_dir') # retina-pretrain path\n",
    "\n",
    "best_last = config.get('testing settings', 'best_last') # use best weights? last weights?\n",
    "\n",
    "name_experiment = 'result/'+'conjunctival_result/' +name_experiment\n",
    "print(name_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare conj data block\n",
    "\n",
    "### Data information  \n",
    "    ch : 1ch (gray level)  \n",
    "    size : All diffent. shit  \n",
    "    information : Conjunctival data from slit lamp. (just one person.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 2336, 3504, 3)\n",
      "(45, 2336, 3504)\n"
     ]
    }
   ],
   "source": [
    "img_shape_list = []\n",
    "img_list= []\n",
    "ground_list =[]\n",
    "\n",
    "# get image's size\n",
    "# image shape -> H, W, C \n",
    "for count, filename in enumerate(sorted(os.listdir(path_imgs)), start=1):\n",
    "    if filename.startswith(\".ipynb\") == False:\n",
    "        #print('Enter {} to select {}'.format(count, filename))\n",
    "        tempImg = cv2.imread(path_imgs + filename)\n",
    "        #print('[DEBUG] shape of imgs : ', np.shape(tempImg), '\\n')\n",
    "        img_list.append(tempImg)\n",
    "    \n",
    "for count, filename in enumerate(sorted(os.listdir(path_ground_truth)), start=1):\n",
    "    if filename.startswith(\".ipynb\") == False:\n",
    "        #print('Enter {} to select {}'.format(count, filename))\n",
    "        tempImg = cv2.imread(path_ground_truth + filename,0)\n",
    "        #print('[DEBUG] shape of ground truth : ', np.shape(tempImg),'\\n')\n",
    "        ground_list.append(tempImg)\n",
    "        \n",
    "img_list = np.array(img_list)\n",
    "ground_list = np.array(ground_list)\n",
    "\n",
    "print(np.shape(img_list))\n",
    "print(np.shape(ground_list))\n",
    "\n",
    "\n",
    "#show_on_jupyter(img_list[8], color = 'gray')\n",
    "#show_on_jupyter(ground_list[8],color = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist the folder in this path : ./result/conjunctival_result/20_03_20_test01\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir('./'+name_experiment) == False:\n",
    "    os.mkdir('./'+name_experiment)\n",
    "else:\n",
    "    print('already exist the folder in this path : {}'.format('./'+name_experiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentations(patches,masks,angle,ratio = 0.1):\n",
    "    '''\n",
    "    patches\n",
    "        range [0.0, 1.0]\n",
    "        \n",
    "    ratio\n",
    "        determine augmentation ratio (add noise, add blur)\n",
    "        \n",
    "    3 combination\n",
    "        1. gaussian noise\n",
    "        2. rotation\n",
    "        3. gaussian noise + rotation\n",
    "    \n",
    "    '''\n",
    "    sampling_num = int(len(patches) * ratio)\n",
    "    augmentation_patches = patches.transpose(0,2,3,1)\n",
    "    augmentation_masks = masks.transpose(0,2,3,1)\n",
    "    #print('augmentation shape : ', augmentation_patches)\n",
    "    \n",
    "    h,w = augmentation_patches.shape[1], augmentation_patches.shape[2]\n",
    "    \n",
    "    sampling_list = random.sample(range(len(patches)), sampling_num)\n",
    "    s_cnt = 0\n",
    "    \n",
    "    print('[Augmentation function] patches shape : ',np.shape(patches))\n",
    "    print('[Augmentation function] augmentation patches shape : ',np.shape(augmentation_patches))\n",
    "    print('[Augmentation function] augmentation patches masks shape : ',np.shape(augmentation_masks))\n",
    "    \n",
    "    for i in range(sampling_num):\n",
    "        choice_num = random.randint(0,1)\n",
    "        \n",
    "        if choice_num ==0:\n",
    "            #print('[Augmentation loop] aug patches shape : ',np.shape(augmentation_patches[sampling_list[s_cnt]]))\n",
    "            pass\n",
    "            #augmentation_patches[sampling_list[s_cnt]] = add_noise(augmentation_patches[sampling_list[s_cnt]])\n",
    "            \n",
    "        elif choice_num ==1:\n",
    "            #print('[Augmentation loop] aug patches shape : ',np.shape(augmentation_patches[sampling_list[s_cnt]]))\n",
    "            augmentation_patches[sampling_list[s_cnt]],augmentation_masks[sampling_list[s_cnt]] = add_rotate(augmentation_patches[sampling_list[s_cnt]],augmentation_masks[sampling_list[s_cnt]],angle,h,w)\n",
    "            \n",
    "        elif choice_num ==2:\n",
    "            #print('[Augmentation loop] aug patches shape : ',np.shape(augmentation_patches[sampling_list[s_cnt]]))\n",
    "            augmentation_patches[sampling_list[s_cnt]] = add_noise(augmentation_patches[sampling_list[s_cnt]])\n",
    "            augmentation_patches[sampling_list[s_cnt]],augmentation_masks[sampling_list[s_cnt]] = add_rotate(augmentation_patches[sampling_list[s_cnt]],augmentation_masks[sampling_list[s_cnt]],angle,h,w)  \n",
    "           \n",
    "        \n",
    "        elif choice_num ==3:\n",
    "            augmentation_patches[sampling_list[s_cnt]] = add_contrast(augmentation_patches[sampling_list[s_cnt]])\n",
    "        \n",
    "        elif choice_num ==4:\n",
    "            augmentation_patches[sampling_list[s_cnt]] = add_contrast(augmentation_patches[sampling_list[s_cnt]])\n",
    "            augmentation_patches[sampling_list[s_cnt]] = add_noise(augmentation_patches[sampling_list[s_cnt]])\n",
    "        elif choice_num ==5:\n",
    "            augmentation_patches[sampling_list[s_cnt]] = add_contrast(augmentation_patches[sampling_list[s_cnt]])\n",
    "            augmentation_patches[sampling_list[s_cnt]],augmentation_masks[sampling_list[s_cnt]] = add_rotate(augmentation_patches[sampling_list[s_cnt]],augmentation_masks[sampling_list[s_cnt]],angle,h,w)\n",
    "        elif choice_num ==6:\n",
    "            augmentation_patches[sampling_list[s_cnt]] = add_contrast(augmentation_patches[sampling_list[s_cnt]])\n",
    "            augmentation_patches[sampling_list[s_cnt]] = add_noise(augmentation_patches[sampling_list[s_cnt]])\n",
    "            augmentation_patches[sampling_list[s_cnt]],augmentation_masks[sampling_list[s_cnt]] = add_rotate(augmentation_patches[sampling_list[s_cnt]],augmentation_masks[sampling_list[s_cnt]],angle,h,w)  \n",
    "            \n",
    "            \n",
    "        s_cnt +=1\n",
    "        \n",
    "    augmentation_patches = augmentation_patches.transpose(0,3,1,2)\n",
    "    augmentation_masks = augmentation_masks.transpose(0,3,1,2)\n",
    "    \n",
    "    return augmentation_patches, augmentation_masks\n",
    "    \n",
    "\n",
    "def add_noise(patches):\n",
    "    '''\n",
    "    patches\n",
    "        range [0.0,1.0]\n",
    "        \n",
    "    not change image's order\n",
    "    '''\n",
    "    rand_num = random.randint(1,4)\n",
    "    mean = 0.0   # some constant\n",
    "    std = 0.00    # some constant (standard deviation)\n",
    "    std = std + (rand_num * 0.01)\n",
    "\n",
    "    noisy_img = patches + (np.random.normal(mean, std, patches.shape))\n",
    "    noisy_img_clipped = np.clip(noisy_img, 0.0, 1.0)\n",
    "    \n",
    "    return noisy_img_clipped\n",
    "\n",
    "def add_rotate(mat,masks, angle ,h,w):\n",
    "    \"\"\"\n",
    "    Rotates an image (angle in degrees) and expands image to avoid cropping\n",
    "    \"\"\"\n",
    "    cnt =0 \n",
    "    height, width = (h,w) # image shape has 3 dimensions\n",
    "    image_center = (width/2, height/2) # getRotationMatrix2D needs coordinates in reverse order (width, height) compared to shape\n",
    "    angle = random.randint(-angle,angle)\n",
    "    \n",
    "    mask_ch = masks.shape[-1]\n",
    "    rotated_masks = np.zeros((h,w,mask_ch))\n",
    "    \n",
    "    #print('mask shape : ', np.shape(rotated_masks))\n",
    "\n",
    "        \n",
    "    rotation_mat = cv2.getRotationMatrix2D(image_center, angle, 1.)\n",
    "\n",
    "    # rotate image with the new bounds and translated rotation matrix\n",
    "    rotated_mat = cv2.warpAffine(mat, rotation_mat, (width, height))\n",
    "    for i in range(mask_ch):\n",
    "        rotated_masks[:,:,i] = cv2.warpAffine(masks[:,:,i], rotation_mat, (width, height))\n",
    "    \n",
    "    #print('mat shape : ', np.shape(rotated_mat))\n",
    "    #print('mask shape : ', np.shape(rotated_masks))\n",
    "    rotated_mat = np.expand_dims(rotated_mat,-1)\n",
    "    \n",
    "    return rotated_mat, rotated_masks\n",
    "\n",
    "    \n",
    "def add_blur(patches):\n",
    "    kernel_size_list = [3,5]\n",
    "    rand_num = random.randint(0,1)\n",
    "    \n",
    "    blur_patches = cv2.GaussianBlur(patches,(kernel_size_list[rand_num],kernel_size_list[rand_num]), cv2.BORDER_DEFAULT)\n",
    "    blur_patches = np.expand_dims(blur_patches, axis=-1)\n",
    "    return blur_patches\n",
    "\n",
    "def add_contrast(patches):\n",
    "    rand_num = random.uniform(0.8,1.3)\n",
    "    rand_gamma= round(rand_num,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 2336, 3504, 3)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n",
      "(2336, 3504)\n"
     ]
    }
   ],
   "source": [
    "def conj_preprocessing(img_list):\n",
    "    img_len, img_list_h, img_list_w = np.shape(img_list)[0],np.shape(img_list)[1], np.shape(img_list)[2]\n",
    "    preprocessed_img = np.zeros((img_len, img_list_h, img_list_w))\n",
    "    pre_img = img_list.copy()\n",
    "    \n",
    "    for i in range(len(img_list)):\n",
    "        '''\n",
    "        RGB to gray\n",
    "        '''\n",
    "        \n",
    "        #preprocessed_img[i] = preprocessed_img[i][:,:,0] * 0.299 + preprocessed_img[i][:,:,1]* 0.587+preprocessed_img[i][:,:,2]*0.114\n",
    "        print(np.shape(preprocessed_img[i]))\n",
    "        print(np.shape(pre_img[i][:,:,1]))\n",
    "        \n",
    "        preprocessed_img[i] = pre_img[i][:,:,1]\n",
    "        #DEBUG block\n",
    "        #show_on_jupyter(preprocessed_img[i],'gray')\n",
    "        #print('[1]', np.shape(preprocessed_img[i]))\n",
    "        \n",
    "        '''\n",
    "        Dataset_normlaized\n",
    "        '''\n",
    "        imgs_normalized = np.empty(preprocessed_img[i].shape)\n",
    "        imgs_std = np.std(preprocessed_img[i])\n",
    "        imgs_mean = np.mean(preprocessed_img[i])\n",
    "        preprocessed_img[i] = (preprocessed_img[i]-imgs_mean)/imgs_std\n",
    "        preprocessed_img[i] = ((preprocessed_img[i] - np.min(preprocessed_img[i])) / (np.max(preprocessed_img[i])-np.min(preprocessed_img[i])))*255\n",
    "    \n",
    "        #show_on_jupyter(preprocessed_img[i],'gray')\n",
    "        #print('[2]', np.shape(preprocessed_img[i]))\n",
    "        '''\n",
    "        Clahe eqaulized\n",
    "        \n",
    "        '''\n",
    "        #clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        #preprocessed_img[i] = clahe.apply(np.array(preprocessed_img[i], dtype = np.uint8))\n",
    "        \n",
    "        #show_on_jupyter(preprocessed_img[i],'gray')\n",
    "        #print('[3]', np.shape(preprocessed_img[i]))\n",
    "        '''\n",
    "        Adjust gamma\n",
    "        \n",
    "        '''\n",
    "        #gamma = 1.2\n",
    "        \n",
    "        #invGamma = 1.0 / gamma\n",
    "        #table = np.array([((j / 255.0) ** invGamma) * 255 for j in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        # apply gamma correction using the lookup table\n",
    "        #preprocessed_img[i] = cv2.LUT(np.array(preprocessed_img[i], dtype = np.uint8), table)\n",
    "        #print('[4]', np.shape(preprocessed_img[i]))\n",
    "        #show_on_jupyter(preprocessed_img[i],'gray')\n",
    "        \n",
    "        preprocessed_img[i] = preprocessed_img[i] / 255.\n",
    "        #show_on_jupyter(preprocessed_img[i],'gray')\n",
    "        #print(np.max(preprocessed_img[i]))\n",
    "        #print('[5]', np.shape(preprocessed_img[i]))\n",
    "    return preprocessed_img\n",
    "\n",
    "# img shape -> H, W, C\n",
    "print(np.shape(img_list))\n",
    "prerprocessed_img_list = conj_preprocessing(img_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2336, 3504)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(prerprocessed_img_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist the folder in this path : result/conjunctival_result/20_03_20_test01\n",
      "[extract random] patches shape : (200000, 1, 64, 64)\n",
      "[Augmentation function] patches shape :  (200000, 1, 64, 64)\n",
      "[Augmentation function] augmentation patches shape :  (200000, 64, 64, 1)\n",
      "[Augmentation function] augmentation patches masks shape :  (200000, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "def extract_random(full_imgs,full_masks, patch_h,patch_w, num_patches):\n",
    "    \n",
    "    patches = np.empty((num_patches, 1 , patch_h, patch_w))\n",
    "    patches_masks = np.empty((num_patches, 1 , patch_h, patch_w))\n",
    "    \n",
    "    print('[extract random] patches shape : {}'.format(patches.shape))\n",
    "    \n",
    "    \n",
    "    for i in range(len(full_imgs)):  # loop, number of image samples\n",
    "        \n",
    "        img_h = full_imgs[i].shape[0]\n",
    "        img_w = full_imgs[i].shape[1]\n",
    "\n",
    "        patch_per_img =int(num_patches / len(full_imgs)) #ex, 48 / 20\n",
    "        \n",
    "        #print('[extract random] patches per full image : {}'.format(patch_per_img))\n",
    "        \n",
    "        iter_total =0 #iter over the total number of patches (num_patches)\n",
    "        k =0\n",
    "        \n",
    "        while k<patch_per_img: #iter, number of patch per img\n",
    "            x_center = random.randint(0+int(patch_w/2) , img_w-int(patch_w/2))\n",
    "            y_center = random.randint(0+int(patch_w/2) , img_h-int(patch_w/2))\n",
    "            #print('[extract random] x_center : {}'.format(x_center))\n",
    "            #print('[extract random] y_center : {}'.format(y_center))\n",
    "            \n",
    "            \n",
    "            patch = full_imgs[i][y_center-int(patch_h/2):y_center+int(patch_h/2),x_center-int(patch_w/2):x_center+int(patch_w/2)] #cropping\n",
    "            patch_mask = full_masks[i][y_center-int(patch_h/2):y_center+int(patch_h/2),x_center-int(patch_w/2):x_center+int(patch_w/2)]\n",
    "            #print('[extract random] patch y : {} to {}'.format(y_center-int(patch_h/2),y_center+int(patch_h/2) ))\n",
    "            #print('[extract random] patch size : {}'.format(np.shape(patch)))\n",
    "            #show_on_jupyter(patch,'gray')\n",
    "            #show_on_jupyter(patch_mask,'gray')\n",
    "            \n",
    "            # 여기 이상한데 shape 확인\n",
    "            # 해당 부분에서 자연스럽게 ch-first 로 바뀌어야 함\n",
    "            #print('patch shape : ', np.shape(patch))\n",
    "            #print(np.shape(patches[iter_total]))\n",
    "            patches[iter_total,0]=patch\n",
    "            patches_masks[iter_total,0]=patch_mask\n",
    "            iter_total +=1   #total\n",
    "            k+=1  #per full_img\n",
    "            \n",
    "    return patches, patches_masks\n",
    "\n",
    "save_path = name_experiment\n",
    "if os.path.isdir(save_path) == False:\n",
    "    os.mkdir(save_path)\n",
    "else:\n",
    "    print('already exist the folder in this path : {}'.format(save_path))\n",
    "\n",
    "    \n",
    "# <===== Extract random patch ======>\n",
    "\n",
    "patches_imgs_train, patches_masks_train = extract_random(prerprocessed_img_list, ground_list,\n",
    "                        patch_h = int(config.get('data attributes', 'patch_height')),\n",
    "                        patch_w = int(config.get('data attributes', 'patch_width')),\n",
    "                        num_patches = int(config.get('training settings', 'num_subimgs')))\n",
    "\n",
    "patches_imgs_train,patches_masks_train = augmentations(patches_imgs_train,patches_masks_train,20,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "[group images func] prev data shape  : (120, 1, 64, 64)\n",
      "[group images func] after data shape :  (120, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 320, 1)\n",
      "[group images func] final total image :  (1600, 320, 1)\n",
      "data shape :  (1600, 320, 1)\n",
      "<PIL.Image.Image image mode=L size=320x1600 at 0x7FE91E6A0208>\n",
      "file name :  ./result/conjunctival_result/20_03_20_test01/sample_input_imgs\n",
      "[group images func] prev data shape  : (120, 1, 64, 64)\n",
      "[group images func] after data shape :  (120, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 320, 1)\n",
      "[group images func] final total image :  (1600, 320, 1)\n",
      "data shape :  (1600, 320, 1)\n",
      "<PIL.Image.Image image mode=L size=320x1600 at 0x7FE91E674D30>\n",
      "file name :  ./result/conjunctival_result/20_03_20_test01/sample_input_masks\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=320x1600 at 0x7FE91E674D30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_sample = min(patches_imgs_train.shape[0],120)\n",
    "print(N_sample)\n",
    "visualize(group_images(patches_imgs_train[0:N_sample,:,:,:],5),'./'+name_experiment+'/'+\"sample_input_imgs\")#.show()\n",
    "visualize(group_images(patches_masks_train[0:N_sample,:,:,:],5),'./'+name_experiment+'/'+\"sample_input_masks\")#.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train block  \n",
    "###  Fine tuning block  \n",
    "U-Net fine tuning  \n",
    "  |_ DRIVE dataset (Pretrained)  \n",
    "  |_ Conjunctival dataset (Fine tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'result/conjunctival_result/20_03_20_test01'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + K.epsilon()) / (K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20_02_25_HRF_conj_scaleTest\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered unet\n",
      "input shape :  (None, 1, 64, 64)\n",
      "shape x,g  (None, 64, 16, 16) (None, 128, 8, 8)\n",
      "inter shape :   64\n",
      "stride x : 1 stride y : 1\n",
      "theta_x shape :  (None, 64, 8, 8)\n",
      "upsample_g shape :  (None, 64, 8, 8)\n",
      "shape x,g  (None, 32, 32, 32) (None, 96, 16, 16)\n",
      "inter shape :   32\n",
      "stride x : 1 stride y : 1\n",
      "theta_x shape :  (None, 32, 16, 16)\n",
      "upsample_g shape :  (None, 32, 16, 16)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 64, 64)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 64, 64)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 64, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 64, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 64, 64)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 64, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 32, 32)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   9248        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 16, 16)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 16, 16)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 16, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 16, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 16, 16)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 16, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 16, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 8, 8)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 8, 8)    73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 8, 8)    32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 8, 8)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 128, 8, 8)    147584      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 128, 8, 8)    32          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128, 8, 8)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "gating01_conv (Conv2D)          (None, 128, 8, 8)    16512       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gating01_bn (BatchNormalization (None, 128, 8, 8)    32          gating01_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating01_act (Activation)       (None, 128, 8, 8)    0           gating01_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 8, 8)     8256        gating01_act[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "g_upattn01 (Conv2DTranspose)    (None, 64, 8, 8)     36928       conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "xlattn01 (Conv2D)               (None, 64, 8, 8)     16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 8, 8)     0           g_upattn01[0][0]                 \n",
      "                                                                 xlattn01[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 8, 8)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "psiattn01 (Conv2D)              (None, 1, 8, 8)      65          activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 1, 8, 8)      0           psiattn01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 1, 16, 16)    0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "psi_upattn01 (Lambda)           (None, 64, 16, 16)   0           up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_attnattn01 (Multiply)         (None, 64, 16, 16)   0           psi_upattn01[0][0]               \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_convattn01 (Conv2D)      (None, 64, 16, 16)   4160        q_attnattn01[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 16, 16)   36896       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_bnattn01 (BatchNormaliza (None, 64, 16, 16)   64          q_attn_convattn01[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96, 16, 16)   0           conv2d_transpose_1[0][0]         \n",
      "                                                                 q_attn_bnattn01[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gating02_conv (Conv2D)          (None, 96, 16, 16)   9312        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating02_bn (BatchNormalization (None, 96, 16, 16)   64          gating02_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating02_act (Activation)       (None, 96, 16, 16)   0           gating02_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 16, 16)   3104        gating02_act[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "g_upattn02 (Conv2DTranspose)    (None, 32, 16, 16)   9248        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "xlattn02 (Conv2D)               (None, 32, 16, 16)   4128        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 16, 16)   0           g_upattn02[0][0]                 \n",
      "                                                                 xlattn02[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 16, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "psiattn02 (Conv2D)              (None, 1, 16, 16)    33          activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 1, 16, 16)    0           psiattn02[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 1, 32, 32)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "psi_upattn02 (Lambda)           (None, 32, 32, 32)   0           up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_attnattn02 (Multiply)         (None, 32, 32, 32)   0           psi_upattn02[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_convattn02 (Conv2D)      (None, 32, 32, 32)   1056        q_attnattn02[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 16, 32, 32)   13840       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_bnattn02 (BatchNormaliza (None, 32, 32, 32)   128         q_attn_convattn02[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 48, 32, 32)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 q_attn_bnattn02[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 16, 64, 64)   6928        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 64, 64)   0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 2, 64, 64)    66          concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 2, 4096)      0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 4096, 2)      0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 4096, 2)      0           permute_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 461,460\n",
      "Trainable params: 460,836\n",
      "Non-trainable params: 624\n",
      "__________________________________________________________________________________________________\n",
      "Check: final output of the network:\n",
      "(None, 4096, 2)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 64, 64)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 64, 64)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 64, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 64, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 64, 64)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 64, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 32, 32)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   9248        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 16, 16)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 16, 16)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 16, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 16, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 16, 16)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 16, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 16, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 8, 8)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 8, 8)    73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 8, 8)    32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 8, 8)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 128, 8, 8)    147584      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 128, 8, 8)    32          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128, 8, 8)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "gating01_conv (Conv2D)          (None, 128, 8, 8)    16512       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gating01_bn (BatchNormalization (None, 128, 8, 8)    32          gating01_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating01_act (Activation)       (None, 128, 8, 8)    0           gating01_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 8, 8)     8256        gating01_act[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "g_upattn01 (Conv2DTranspose)    (None, 64, 8, 8)     36928       conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "xlattn01 (Conv2D)               (None, 64, 8, 8)     16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 8, 8)     0           g_upattn01[0][0]                 \n",
      "                                                                 xlattn01[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 8, 8)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "psiattn01 (Conv2D)              (None, 1, 8, 8)      65          activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 1, 8, 8)      0           psiattn01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 1, 16, 16)    0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "psi_upattn01 (Lambda)           (None, 64, 16, 16)   0           up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_attnattn01 (Multiply)         (None, 64, 16, 16)   0           psi_upattn01[0][0]               \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_convattn01 (Conv2D)      (None, 64, 16, 16)   4160        q_attnattn01[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 16, 16)   36896       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_bnattn01 (BatchNormaliza (None, 64, 16, 16)   64          q_attn_convattn01[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96, 16, 16)   0           conv2d_transpose_1[0][0]         \n",
      "                                                                 q_attn_bnattn01[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gating02_conv (Conv2D)          (None, 96, 16, 16)   9312        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating02_bn (BatchNormalization (None, 96, 16, 16)   64          gating02_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating02_act (Activation)       (None, 96, 16, 16)   0           gating02_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 16, 16)   3104        gating02_act[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "g_upattn02 (Conv2DTranspose)    (None, 32, 16, 16)   9248        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "xlattn02 (Conv2D)               (None, 32, 16, 16)   4128        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 16, 16)   0           g_upattn02[0][0]                 \n",
      "                                                                 xlattn02[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 16, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "psiattn02 (Conv2D)              (None, 1, 16, 16)    33          activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 1, 16, 16)    0           psiattn02[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 1, 32, 32)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "psi_upattn02 (Lambda)           (None, 32, 32, 32)   0           up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_attnattn02 (Multiply)         (None, 32, 32, 32)   0           psi_upattn02[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_convattn02 (Conv2D)      (None, 32, 32, 32)   1056        q_attnattn02[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 16, 32, 32)   13840       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_bnattn02 (BatchNormaliza (None, 32, 32, 32)   128         q_attn_convattn02[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 48, 32, 32)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 q_attn_bnattn02[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 16, 64, 64)   6928        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 64, 64)   0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 2, 64, 64)    66          concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 2, 4096)      0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 4096, 2)      0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 4096, 2)      0           permute_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 461,460\n",
      "Trainable params: 460,836\n",
      "Non-trainable params: 624\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "n_ch = patches_imgs_train.shape[1]\n",
    "patch_height = patches_imgs_train.shape[2]\n",
    "patch_width = patches_imgs_train.shape[3]\n",
    "\n",
    "if fine_tuning_flag==\"False\":\n",
    "    print (\"entered unet\")\n",
    "    model = small_attn_unet(n_ch, patch_height, patch_width,2)\n",
    "    #model = unet_norm(n_ch, patch_height, patch_width)  #the U-net model\n",
    "    #model = model_from_json(open('./result/Retina_Result/'+ pretrained_dir + '/' + pretrained_dir +'_architecture.json').read())\n",
    "    #model.load_weights('./result/Retina_Result/'+ pretrained_dir + '/' +best_last+'_weights.h5')\n",
    "    \n",
    "    plot(model, to_file='./'+name_experiment+'/' + '_model.png')   #check how the model looks like\n",
    "    json_string = model.to_json()\n",
    "    open('./'+name_experiment+'/'+'_architecture.json', 'w').write(json_string)\n",
    "\n",
    "elif fine_tuning_flag==\"True\":\n",
    "    # --------------------Fine tuning while training -----------------\n",
    "    print (\"About to load the pre trained model for fine tuning\")\n",
    "    model = model_from_json(open('./result/Retina_Result/'+ pretrained_dir + '/' + pretrained_dir +'_architecture.json').read())\n",
    "    model.load_weights('./result/Retina_Result/'+ pretrained_dir + '/' +best_last+'_weights.h5')\n",
    "    print (\"loaded the pre trained model \\n\\n\")\n",
    "\n",
    "    \n",
    "    for layer in model.layers[:]:\n",
    "        if layer.name =='conv2d_11':\n",
    "            layer.trainable = True\n",
    "            print (layer.name,'\\t',\"Trainable layer\")\n",
    "        else :\n",
    "            layer.trainable = False\n",
    "        \n",
    "        print (layer.name,'\\t',\"these layers wont be trained\")\n",
    "    \n",
    "    ''' \n",
    "    for layer in model.layers[17:25]:\n",
    "        layer.trainable = False\n",
    "        print (layer.name,'\\t',\"these layers wont be trained\")\n",
    "    '''\n",
    "    #sgd = SGD(lr=0.0001, momentum=0., decay=0., nesterov=False)\n",
    "    adam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "   # model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.compile(optimizer=adam, loss=dice_coef_loss,metrics=[dice_coef])\n",
    "    \n",
    "#print model.layers\n",
    "print (\"Check: final output of the network:\")\n",
    "print (model.output_shape)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "import math\n",
    "\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "    \"\"\"Cosine annealing scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # 1e-9 + (1e-6 ) * (1 + cos ())\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model training block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training session] before mask unet func patch mask shape :  (200000, 1, 64, 64)\n",
      "[training session] after mask unet func patch mask shape :  (200000, 4096, 2)\n",
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.4959 - dice_coef: 0.5041 - val_loss: 0.4914 - val_dice_coef: 0.5086\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.49592, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 2/150\n",
      "160000/160000 [==============================] - 185s 1ms/step - loss: 0.4385 - dice_coef: 0.5615 - val_loss: 0.4450 - val_dice_coef: 0.5550\n",
      "\n",
      "Epoch 00002: loss improved from 0.49592 to 0.43853, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 3/150\n",
      "160000/160000 [==============================] - 186s 1ms/step - loss: 0.3870 - dice_coef: 0.6130 - val_loss: 0.3945 - val_dice_coef: 0.6055\n",
      "\n",
      "Epoch 00003: loss improved from 0.43853 to 0.38702, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 4/150\n",
      "160000/160000 [==============================] - 187s 1ms/step - loss: 0.3518 - dice_coef: 0.6482 - val_loss: 0.3633 - val_dice_coef: 0.6367\n",
      "\n",
      "Epoch 00004: loss improved from 0.38702 to 0.35175, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 5/150\n",
      "160000/160000 [==============================] - 186s 1ms/step - loss: 0.3208 - dice_coef: 0.6792 - val_loss: 0.3320 - val_dice_coef: 0.6680\n",
      "\n",
      "Epoch 00005: loss improved from 0.35175 to 0.32084, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 6/150\n",
      "160000/160000 [==============================] - 188s 1ms/step - loss: 0.2939 - dice_coef: 0.7061 - val_loss: 0.2858 - val_dice_coef: 0.7142\n",
      "\n",
      "Epoch 00006: loss improved from 0.32084 to 0.29391, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 7/150\n",
      "160000/160000 [==============================] - 186s 1ms/step - loss: 0.2605 - dice_coef: 0.7395 - val_loss: 0.2406 - val_dice_coef: 0.7594\n",
      "\n",
      "Epoch 00007: loss improved from 0.29391 to 0.26054, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 8/150\n",
      "160000/160000 [==============================] - 186s 1ms/step - loss: 0.2195 - dice_coef: 0.7805 - val_loss: 0.2009 - val_dice_coef: 0.7991\n",
      "\n",
      "Epoch 00008: loss improved from 0.26054 to 0.21954, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 9/150\n",
      "160000/160000 [==============================] - 186s 1ms/step - loss: 0.1774 - dice_coef: 0.8226 - val_loss: 0.1666 - val_dice_coef: 0.8334\n",
      "\n",
      "Epoch 00009: loss improved from 0.21954 to 0.17737, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 10/150\n",
      "160000/160000 [==============================] - 186s 1ms/step - loss: 0.1523 - dice_coef: 0.8477 - val_loss: 0.1478 - val_dice_coef: 0.8522\n",
      "\n",
      "Epoch 00010: loss improved from 0.17737 to 0.15227, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 11/150\n",
      "160000/160000 [==============================] - 188s 1ms/step - loss: 0.1154 - dice_coef: 0.8846 - val_loss: 0.1083 - val_dice_coef: 0.8917\n",
      "\n",
      "Epoch 00011: loss improved from 0.15227 to 0.11544, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 12/150\n",
      "160000/160000 [==============================] - 184s 1ms/step - loss: 0.0908 - dice_coef: 0.9092 - val_loss: 0.0791 - val_dice_coef: 0.9209\n",
      "\n",
      "Epoch 00012: loss improved from 0.11544 to 0.09077, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 13/150\n",
      "160000/160000 [==============================] - 183s 1ms/step - loss: 0.0667 - dice_coef: 0.9333 - val_loss: 0.0613 - val_dice_coef: 0.9387\n",
      "\n",
      "Epoch 00013: loss improved from 0.09077 to 0.06668, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 14/150\n",
      "160000/160000 [==============================] - 183s 1ms/step - loss: 0.0330 - dice_coef: 0.9670 - val_loss: 0.0199 - val_dice_coef: 0.9801\n",
      "\n",
      "Epoch 00014: loss improved from 0.06668 to 0.03304, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 15/150\n",
      "160000/160000 [==============================] - 183s 1ms/step - loss: 0.0160 - dice_coef: 0.9840 - val_loss: 0.0107 - val_dice_coef: 0.9893\n",
      "\n",
      "Epoch 00015: loss improved from 0.03304 to 0.01597, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 16/150\n",
      "160000/160000 [==============================] - 183s 1ms/step - loss: 0.0098 - dice_coef: 0.9902 - val_loss: 0.0057 - val_dice_coef: 0.9943\n",
      "\n",
      "Epoch 00016: loss improved from 0.01597 to 0.00976, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 17/150\n",
      "160000/160000 [==============================] - 182s 1ms/step - loss: 0.0063 - dice_coef: 0.9937 - val_loss: 0.0031 - val_dice_coef: 0.9969\n",
      "\n",
      "Epoch 00017: loss improved from 0.00976 to 0.00631, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 18/150\n",
      "160000/160000 [==============================] - 182s 1ms/step - loss: 0.0044 - dice_coef: 0.9956 - val_loss: 0.0017 - val_dice_coef: 0.9983\n",
      "\n",
      "Epoch 00018: loss improved from 0.00631 to 0.00443, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 19/150\n",
      "160000/160000 [==============================] - 181s 1ms/step - loss: 0.0034 - dice_coef: 0.9966 - val_loss: 9.2512e-04 - val_dice_coef: 0.9991\n",
      "\n",
      "Epoch 00019: loss improved from 0.00443 to 0.00339, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 20/150\n",
      "160000/160000 [==============================] - 181s 1ms/step - loss: 0.0028 - dice_coef: 0.9972 - val_loss: 5.2178e-04 - val_dice_coef: 0.9995\n",
      "\n",
      "Epoch 00020: loss improved from 0.00339 to 0.00281, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 21/150\n",
      "160000/160000 [==============================] - 181s 1ms/step - loss: 0.0025 - dice_coef: 0.9975 - val_loss: 2.8813e-04 - val_dice_coef: 0.9997\n",
      "\n",
      "Epoch 00021: loss improved from 0.00281 to 0.00251, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 22/150\n",
      "160000/160000 [==============================] - 181s 1ms/step - loss: 0.0024 - dice_coef: 0.9976 - val_loss: 1.4943e-04 - val_dice_coef: 0.9999\n",
      "\n",
      "Epoch 00022: loss improved from 0.00251 to 0.00235, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 23/150\n",
      "160000/160000 [==============================] - 181s 1ms/step - loss: 0.0023 - dice_coef: 0.9977 - val_loss: 7.2420e-05 - val_dice_coef: 0.9999\n",
      "\n",
      "Epoch 00023: loss improved from 0.00235 to 0.00227, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 24/150\n",
      "160000/160000 [==============================] - 181s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 3.2902e-05 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00024: loss improved from 0.00227 to 0.00223, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 25/150\n",
      "160000/160000 [==============================] - 180s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 1.5080e-05 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00025: loss improved from 0.00223 to 0.00221, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 26/150\n",
      "160000/160000 [==============================] - 180s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 6.9141e-06 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00026: loss improved from 0.00221 to 0.00220, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 27/150\n",
      "160000/160000 [==============================] - 180s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 3.1590e-06 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00027: loss improved from 0.00220 to 0.00220, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 28/150\n",
      "160000/160000 [==============================] - 180s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 1.5497e-06 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00028: loss improved from 0.00220 to 0.00220, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 29/150\n",
      "160000/160000 [==============================] - 180s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 6.5565e-07 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00029: loss improved from 0.00220 to 0.00220, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 30/150\n",
      "160000/160000 [==============================] - 180s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 2.3842e-07 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00030: loss improved from 0.00220 to 0.00219, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 31/150\n",
      "160000/160000 [==============================] - 180s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 1.1921e-07 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00031: loss improved from 0.00219 to 0.00219, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 32/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00032: loss improved from 0.00219 to 0.00219, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 33/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00033: loss improved from 0.00219 to 0.00219, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 34/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00034: loss improved from 0.00219 to 0.00219, saving model to ./result/conjunctival_result/20_03_20_test01/best_weights.h5\n",
      "Epoch 35/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.00219\n",
      "Epoch 36/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.00219\n",
      "Epoch 37/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.00219\n",
      "Epoch 38/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.00219\n",
      "Epoch 39/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.00219\n",
      "Epoch 40/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.00219\n",
      "Epoch 41/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.00219\n",
      "Epoch 42/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.00219\n",
      "Epoch 43/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.00219\n",
      "Epoch 44/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.00219\n",
      "Epoch 45/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.00219\n",
      "Epoch 46/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.00219\n",
      "Epoch 47/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.00219\n",
      "Epoch 48/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00048: loss did not improve from 0.00219\n",
      "Epoch 49/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.00219\n",
      "Epoch 50/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00050: loss did not improve from 0.00219\n",
      "Epoch 51/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.00219\n",
      "Epoch 52/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00052: loss did not improve from 0.00219\n",
      "Epoch 53/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.00219\n",
      "Epoch 54/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00054: loss did not improve from 0.00219\n",
      "Epoch 55/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.00219\n",
      "Epoch 56/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.00219\n",
      "Epoch 57/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00057: loss did not improve from 0.00219\n",
      "Epoch 58/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.00219\n",
      "Epoch 59/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.00219\n",
      "Epoch 60/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.00219\n",
      "Epoch 61/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.00219\n",
      "Epoch 62/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.00219\n",
      "Epoch 63/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00063: loss did not improve from 0.00219\n",
      "Epoch 64/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.00219\n",
      "Epoch 65/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.00219\n",
      "Epoch 66/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.00219\n",
      "Epoch 67/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.00219\n",
      "Epoch 68/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.00219\n",
      "Epoch 69/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.00219\n",
      "Epoch 70/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.00219\n",
      "Epoch 71/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00071: loss did not improve from 0.00219\n",
      "Epoch 72/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00072: loss did not improve from 0.00219\n",
      "Epoch 73/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.00219\n",
      "Epoch 74/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.00219\n",
      "Epoch 75/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.00219\n",
      "Epoch 76/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00076: loss did not improve from 0.00219\n",
      "Epoch 77/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.00219\n",
      "Epoch 78/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.00219\n",
      "Epoch 79/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.00219\n",
      "Epoch 80/150\n",
      "160000/160000 [==============================] - 180s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.00219\n",
      "Epoch 81/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.00219\n",
      "Epoch 82/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.00219\n",
      "Epoch 83/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.00219\n",
      "Epoch 84/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.00219\n",
      "Epoch 85/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00085: loss did not improve from 0.00219\n",
      "Epoch 86/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.00219\n",
      "Epoch 87/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.00219\n",
      "Epoch 88/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.00219\n",
      "Epoch 89/150\n",
      "160000/160000 [==============================] - 179s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.00219\n",
      "Epoch 90/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00090: loss did not improve from 0.00219\n",
      "Epoch 91/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00091: loss did not improve from 0.00219\n",
      "Epoch 92/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.00219\n",
      "Epoch 93/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00093: loss did not improve from 0.00219\n",
      "Epoch 94/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.00219\n",
      "Epoch 95/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.00219\n",
      "Epoch 96/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.00219\n",
      "Epoch 97/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00097: loss did not improve from 0.00219\n",
      "Epoch 98/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00098: loss did not improve from 0.00219\n",
      "Epoch 99/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.00219\n",
      "Epoch 100/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.00219\n",
      "Epoch 101/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00101: loss did not improve from 0.00219\n",
      "Epoch 102/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00102: loss did not improve from 0.00219\n",
      "Epoch 103/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00103: loss did not improve from 0.00219\n",
      "Epoch 104/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00104: loss did not improve from 0.00219\n",
      "Epoch 105/150\n",
      "160000/160000 [==============================] - 177s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00105: loss did not improve from 0.00219\n",
      "Epoch 106/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00106: loss did not improve from 0.00219\n",
      "Epoch 107/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00107: loss did not improve from 0.00219\n",
      "Epoch 108/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00108: loss did not improve from 0.00219\n",
      "Epoch 109/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00109: loss did not improve from 0.00219\n",
      "Epoch 110/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00110: loss did not improve from 0.00219\n",
      "Epoch 111/150\n",
      "160000/160000 [==============================] - 177s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00111: loss did not improve from 0.00219\n",
      "Epoch 112/150\n",
      "160000/160000 [==============================] - 177s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00112: loss did not improve from 0.00219\n",
      "Epoch 113/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00113: loss did not improve from 0.00219\n",
      "Epoch 114/150\n",
      "160000/160000 [==============================] - 177s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00114: loss did not improve from 0.00219\n",
      "Epoch 115/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00115: loss did not improve from 0.00219\n",
      "Epoch 116/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00116: loss did not improve from 0.00219\n",
      "Epoch 117/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00117: loss did not improve from 0.00219\n",
      "Epoch 118/150\n",
      "160000/160000 [==============================] - 177s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00118: loss did not improve from 0.00219\n",
      "Epoch 119/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00119: loss did not improve from 0.00219\n",
      "Epoch 120/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00120: loss did not improve from 0.00219\n",
      "Epoch 121/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00121: loss did not improve from 0.00219\n",
      "Epoch 122/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00122: loss did not improve from 0.00219\n",
      "Epoch 123/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00123: loss did not improve from 0.00219\n",
      "Epoch 124/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00124: loss did not improve from 0.00219\n",
      "Epoch 125/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00125: loss did not improve from 0.00219\n",
      "Epoch 126/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00126: loss did not improve from 0.00219\n",
      "Epoch 127/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00127: loss did not improve from 0.00219\n",
      "Epoch 128/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00128: loss did not improve from 0.00219\n",
      "Epoch 129/150\n",
      "160000/160000 [==============================] - 177s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00129: loss did not improve from 0.00219\n",
      "Epoch 130/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00130: loss did not improve from 0.00219\n",
      "Epoch 131/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00131: loss did not improve from 0.00219\n",
      "Epoch 132/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00132: loss did not improve from 0.00219\n",
      "Epoch 133/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00133: loss did not improve from 0.00219\n",
      "Epoch 134/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00134: loss did not improve from 0.00219\n",
      "Epoch 135/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00135: loss did not improve from 0.00219\n",
      "Epoch 136/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00136: loss did not improve from 0.00219\n",
      "Epoch 137/150\n",
      "160000/160000 [==============================] - 177s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00137: loss did not improve from 0.00219\n",
      "Epoch 138/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00138: loss did not improve from 0.00219\n",
      "Epoch 139/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00139: loss did not improve from 0.00219\n",
      "Epoch 140/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00140: loss did not improve from 0.00219\n",
      "Epoch 141/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00141: loss did not improve from 0.00219\n",
      "Epoch 142/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00142: loss did not improve from 0.00219\n",
      "Epoch 143/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00143: loss did not improve from 0.00219\n",
      "Epoch 144/150\n",
      "160000/160000 [==============================] - 177s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00144: loss did not improve from 0.00219\n",
      "Epoch 145/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00145: loss did not improve from 0.00219\n",
      "Epoch 146/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00146: loss did not improve from 0.00219\n",
      "Epoch 147/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00147: loss did not improve from 0.00219\n",
      "Epoch 148/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00148: loss did not improve from 0.00219\n",
      "Epoch 149/150\n",
      "160000/160000 [==============================] - 178s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00149: loss did not improve from 0.00219\n",
      "Epoch 150/150\n",
      "160000/160000 [==============================] - 177s 1ms/step - loss: 0.0022 - dice_coef: 0.9978 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "\n",
      "Epoch 00150: loss did not improve from 0.00219\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Keras provides a set of functions called callbacks: \n",
    "you can think of callbacks as events that will be triggered at certain training states. \n",
    "The callback we need for checkpointing is the ModelCheckpoint \n",
    "which provides all the features we need according to the checkpointing strategy we adopted in our example\n",
    "'''\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./'+name_experiment+'/best_weights.h5', verbose=1, monitor='loss', mode='auto', save_best_only=True) #save at each epoch if the validation decreased\n",
    "\n",
    "print('[training session] before mask unet func patch mask shape : ',patches_masks_train.shape)\n",
    "patches_masks_train = masks_Unet(patches_masks_train)  #reduce memory consumption\n",
    "print('[training session] after mask unet func patch mask shape : ',patches_masks_train.shape)\n",
    "\n",
    "try:\n",
    "    history = model.fit(patches_imgs_train, patches_masks_train, epochs=num_epochs, batch_size=batch_size, verbose=1, shuffle=True, validation_split=0.2, callbacks=[checkpointer, CosineAnnealingScheduler(T_max=150, eta_max=1e-6, eta_min=1e-8)])\n",
    "    model.save_weights('./'+name_experiment +'/last_weights.h5', overwrite=True)\n",
    "except KeyboardInterrupt:\n",
    "    model.save_weights('./'+name_experiment +'/last_weights.h5', overwrite=True)\n",
    "    print('Keyboard Interrupt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-06"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-820f85b80291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "print(history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-248db6af0ccd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dice_coef'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss= history.history['val_loss']\n",
    "acc = history.history['dice_coef']\n",
    "val_acc = history.history['val_dice_coef']\n",
    "\n",
    "epochs = range(1,len(acc) +1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label = \"Training loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label = \"Validation loss\")\n",
    "plt.title(\"Training and Validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.savefig(name_experiment+\"/training_loss_result.png\")\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'ro', label = \"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, 'r', label = \"Validation accuracy\")\n",
    "plt.title(\"Training and Validation acc\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig(name_experiment+\"/training_acc_result.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = './'+name_experiment + '/' + 'train_history'\n",
    "\n",
    "def save_history_txt_csv(history, file_path, file_name):\n",
    "    if os.path.isdir(file_path) == False:\n",
    "        os.mkdir(file_path)\n",
    "    else:\n",
    "        print('already exist the folder in this path : {}'.format(file_path))\n",
    "    \n",
    "    hist_df = pd.DataFrame(history) \n",
    "\n",
    "    # save to json:  \n",
    "    hist_json_file = file_path + '/' + file_name +'.json' \n",
    "    with open(hist_json_file, mode='w') as f:\n",
    "        hist_df.to_json(f)\n",
    "\n",
    "    # or save to csv: \n",
    "    hist_csv_file = file_path + '/' + file_name + '.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "        \n",
    "\n",
    "save_history_txt_csv(loss, file_path, 'loss')\n",
    "save_history_txt_csv(acc, file_path, 'dice_coef')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
